{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backpropagation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnY9Rd5VWkTA"
      },
      "source": [
        "# B\"H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQhyUTvpWmHQ"
      },
      "source": [
        "## Singel Neuron \"Network\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0CRppjbW1HV"
      },
      "source": [
        "Before applying this to a complete neural network, let’s start with a simplified forward pass with just one neuron. Rather than backpropagating from the loss function for a full neural network, let’s backpropagate the ReLU function for a single neuron and act as if we intend to **minimize the output for this single neuron**. \n",
        "\n",
        "This example is obviously not used in the real world (where we minimize the loss etc) - this just for learning purposes etc.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1_lq5wWBDiqhtXbPaOTfvdCrZY0o7qJQz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPtUyj5XXC4-",
        "outputId": "a8837769-c5f4-40f5-b3be-ffc22200dc03"
      },
      "source": [
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "print(xw0, xw1, xw2, b)\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "print(z)\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "print(y)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-3.0 2.0 6.0 1.0\n",
            "6.0\n",
            "6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug_oYdClXgWJ"
      },
      "source": [
        "### Our Big Function\n",
        "\n",
        "![](https://drive.google.com/uc?id=1JpU0VqoiiRBLoDyrV7GE1JSED14cwPxo)\n",
        "\n",
        "<br>\n",
        "\n",
        "Let’s rewrite our equation to the form that will allow us to determine how to calculate the derivatives more easily:\n",
        "\n",
        "![](https://drive.google.com/uc?id=17XPz2uAdSPCmTwHn6D0b5I6BDCHZudmP)\n",
        "\n",
        "<br>\n",
        "\n",
        "... in psuedo-code:\n",
        "\n",
        "```\n",
        "ReLU(\n",
        "    sum(\n",
        "        mul(x0, w0), \n",
        "        mul(x1, w1), \n",
        "        mul(x2, w2), \n",
        "        b\n",
        "    )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynyjtkGIZyFj"
      },
      "source": [
        "### Partial derivative of w0\n",
        "\n",
        "Let's start by considering what we need to calculate for the partial derivative of $\\large w_0$\n",
        "\n",
        "![](https://drive.google.com/uc?id=1JdTSOrQXda3c6a6LOoFHZRUB2HnFP4X5)\n",
        "\n",
        "\n",
        "> For legibility, we did not denote the ReLU() parameter, which is the full sum, and the sum parameters, which are all of the multiplications of inputs and weights. We excluded this because the equation would be longer and harder to read. \n",
        "\n",
        "This equation shows that we have to calculate the derivatives and partial derivatives of all of the atomic operations and multiply them to acquire the impact that $\\large x_0$ makes on the output. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74_03ZrpbLvS"
      },
      "source": [
        "#### Gradient from next layer\n",
        "\n",
        "We’ll have multiple chained layers of neurons in the neural network model, followed by the loss function. \n",
        "\n",
        "We'll want to know the impact of a given **weight or bias** on the loss. \n",
        "\n",
        "The derivative **with respect to the layer’s inputs**, as opposed to the derivative **with respect to the weights and biases**, is not used to update any parameters. Instead, it is used to **chain** to another layer (which is why we backpropagate to the previous layer in a chain).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For this example, let’s assume that our neuron receives a gradient of $1$ from the **next layer**. We’re making up this value for demonstration purposes, and a value of 1 won’t change the values, which means that we can more easily show all of the processes. \n",
        "\n",
        "We are going to use the color of red for derivatives.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1kdRsvEPwknm-mTohSuPzUrv2S6MCYml-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10YzeN2ndbKQ"
      },
      "source": [
        "#### ReLU derivative\n",
        "\n",
        "Recall that the derivative of ReLU() **with respect to its input** is 1, if the input is greater than 0, and 0 otherwise.\n",
        "\n",
        "The input value to the ReLU function is 6, so the derivative equals 1. \n",
        "\n",
        "We have to use the chain rule and multiply this derivative with the derivative received from the next layer (which we made up to be 1).\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://drive.google.com/uc?id=1ZBCFRJbpnniz3uZO5vEFC0C_xEPs7LXk)\n",
        "\n",
        "This results with the derivative of 1:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1fyBKyZ3Bz-nwGfbgRvbZGFJzojt4fyok)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4yGocJadzDt",
        "outputId": "bf7ca928-ba0a-4706-b178-23a7e50f19f0"
      },
      "source": [
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# The derivative from the next layer\n",
        "dvalue = 1.0\n",
        "\n",
        "# Derivative of ReLU and the chain rule\n",
        "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
        "print(drelu_dz)\n",
        "# -- ---------------------------------------\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q61L1oIe9Vl"
      },
      "source": [
        "#### Sum derivative\n",
        "\n",
        "![](https://drive.google.com/uc?id=1vKEGB1GIt0ob1U9f_UBYpYIZ8xiLm323)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "> Note: `w.r.t` stands for \"with respect to\"\n",
        "\n",
        "---\n",
        "\n",
        "The partial derivative of the simple sum operation (i.e. $f(x, y, z) = x + y + z$) is always 1, no matter the inputs:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1CK16QPZ9QmMF-GQzC44ApNYF2Nm--s7m)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqHyMkFHhnDI",
        "outputId": "4f104a30-f21e-410a-fa86-f0e44016f27f"
      },
      "source": [
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# -- ----------------------\n",
        "# The derivative from the next layer\n",
        "dvalue = 1.0\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Derivative of ReLU with chain rule\n",
        "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
        "print(drelu_dz)\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the sum with chain rule\n",
        "dsum_dxw0 = 1\n",
        "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
        "print(drelu_dxw0)\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ---------------------------------------\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwdNYwm5g0nP"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1IVD0aLm49_FiBe1iodPAc97IMu4r0iRj)\n",
        "\n",
        "Result:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1bLDvheKGVXgleqzgrIBx_tCfC-zoBxkF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2OlmAuZj93O"
      },
      "source": [
        "### For all weighted inputs and bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbQl0ODXkHJc"
      },
      "source": [
        "We can then perform the same operation.\n",
        "\n",
        "![](https://drive.google.com/uc?id=13bXiryB5dBMpa9vsAJIDVBJEHOsSO1UM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et4Fo4w5kfyE",
        "outputId": "4a8a6e5e-1382-4370-ec7d-c3dd6c513916"
      },
      "source": [
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# -- ----------------------\n",
        "# The derivative from the next layer\n",
        "dvalue = 1.0\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Derivative of ReLU and the chain rule\n",
        "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the sum, the chain rule\n",
        "dsum_dxw0 = 1\n",
        "dsum_dxw1 = 1\n",
        "dsum_dxw2 = 1\n",
        "dsum_db = 1\n",
        "\n",
        "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
        "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
        "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
        "drelu_db = drelu_dz * dsum_db\n",
        "\n",
        "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ---------------------------------------\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0 1.0 1.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDfztf39k52T"
      },
      "source": [
        "### Multiplication of weights and inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAD8FKTJkLnY"
      },
      "source": [
        "Continuing backward, the next function is the multiplication of weights and inputs. \n",
        "\n",
        "The derivative for a product is whatever the input is being multiplied by. \n",
        "\n",
        "Recall:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1IAQS4eIH242ZSOLRVK3929RMBosiCx7p)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "![](https://drive.google.com/uc?id=19GdcFSnltR2iSBV2nCL7EMv_YLELhESa)\n",
        "\n",
        "---\n",
        "\n",
        "[Video showing above](https://www.youtube.com/watch?v=_9qHQA30hys)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gv2hEjOlkx5",
        "outputId": "1a71d2e3-c508-4b1e-c292-b3c462dcdfb5"
      },
      "source": [
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# -- ----------------------\n",
        "# The derivative from the next layer\n",
        "dvalue = 1.0\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Derivative of ReLU and the chain rule\n",
        "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
        "# -- ----------------------\n",
        "\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the sum, the chain rule\n",
        "dsum_dxw0 = 1\n",
        "dsum_dxw1 = 1\n",
        "dsum_dxw2 = 1\n",
        "dsum_db = 1\n",
        "\n",
        "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
        "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
        "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
        "drelu_db = drelu_dz * dsum_db\n",
        "# -- ----------------------\n",
        "\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the multiplication, the chain rule\n",
        "dmul_dx0 = w[0]\n",
        "dmul_dx1 = w[1]\n",
        "dmul_dx2 = w[2]\n",
        "\n",
        "dmul_dw0 = x[0]\n",
        "dmul_dw1 = x[1]\n",
        "dmul_dw2 = x[2]\n",
        "\n",
        "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
        "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
        "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
        "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
        "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
        "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
        "\n",
        "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ---------------------------------------\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bG9rBLef--V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUcbEKaGZYg-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUUl3vyJXF4i"
      },
      "source": [
        ""
      ]
    }
  ]
}