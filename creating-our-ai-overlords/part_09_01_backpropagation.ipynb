{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part_09_01_backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQhyUTvpWmHQ"
      },
      "source": [
        "### Singel Neuron \"Network\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0CRppjbW1HV"
      },
      "source": [
        "Before applying this to a complete neural network, let’s start with a simplified forward pass with just one neuron. Rather than backpropagating from the loss function for a full neural network, let’s backpropagate the ReLU function for a single neuron and act as if we intend to **minimize the output for this single neuron**. \n",
        "\n",
        "This example is obviously not used in the real world (where we minimize the loss etc) - this just for learning purposes etc.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1_lq5wWBDiqhtXbPaOTfvdCrZY0o7qJQz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPtUyj5XXC4-"
      },
      "source": [
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'xw0 :{xw0}')\n",
        "print(f'xw1 :{xw1}')\n",
        "print(f'xw2 :{xw2}')\n",
        "print(f'z   :{z}')\n",
        "print(f'y   :{y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dp8HzddnT1V",
        "outputId": "ff7daf80-d987-4ed6-e5a8-685507f0f5cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xw0 :-3.0\n",
            "xw1 :2.0\n",
            "xw2 :6.0\n",
            "z   :6.0\n",
            "y   :6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug_oYdClXgWJ"
      },
      "source": [
        "### Our Big Function\n",
        "\n",
        "![](https://drive.google.com/uc?id=1JpU0VqoiiRBLoDyrV7GE1JSED14cwPxo)\n",
        "\n",
        "<br>\n",
        "\n",
        "Let’s rewrite our equation to the form that will allow us to determine how to calculate the derivatives more easily:\n",
        "\n",
        "![](https://drive.google.com/uc?id=17XPz2uAdSPCmTwHn6D0b5I6BDCHZudmP)\n",
        "\n",
        "<br>\n",
        "\n",
        "... in psuedo-code:\n",
        "\n",
        "```\n",
        "ReLU(\n",
        "    sum(\n",
        "        mul(x0, w0), \n",
        "        mul(x1, w1), \n",
        "        mul(x2, w2), \n",
        "        b\n",
        "    )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain Rule"
      ],
      "metadata": {
        "id": "oSxj8V1Aon4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to backpropagate our gradients by calculating derivatives and partial derivatives with respect to each of our parameters and inputs. \n",
        "\n",
        "To do this, we’re going to use the chain rule. \n",
        "\n",
        "Recall that the chain rule for a function stipulates that the derivative for nested functions like $f(g(x))$ solves to:\n",
        "\n",
        "![](https://drive.google.com/uc?id=11PTgdp9-JZgTLzrR9qunoyQGClp5nAar)"
      ],
      "metadata": {
        "id": "Vg7hmbZTop80"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynyjtkGIZyFj"
      },
      "source": [
        "### Partial derivative of x0\n",
        "\n",
        "Let's start by considering what we need to calculate for the partial derivative of $\\large x_0$\n",
        "\n",
        "![](https://drive.google.com/uc?id=1JdTSOrQXda3c6a6LOoFHZRUB2HnFP4X5)\n",
        "\n",
        "\n",
        "> For legibility, we did not denote the $\\large ReLU()$ parameter (which is the full sum), nor the $\\large sum()$ parameters (which are all of the multiplications of inputs and weights). We excluded this because the equation would be longer and harder to read. \n",
        "\n",
        "This equation shows that we have to calculate the derivatives and partial derivatives of all of the atomic operations and multiply them to acquire the impact that $\\large x_0$ makes on the output. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74_03ZrpbLvS"
      },
      "source": [
        "#### Gradient from next layer\n",
        "\n",
        "We’ll have multiple chained layers of neurons in the neural network model, followed by the loss function. \n",
        "\n",
        "We'll want to know the impact of a given **weight or bias** on the loss. \n",
        "\n",
        "The derivative **with respect to the layer’s inputs**, as opposed to the derivative **with respect to the weights and biases**, is not used to update any parameters. Instead, it is used to **chain** to another layer (which is why we backpropagate to the previous layer in a chain).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For this example, let’s assume that our neuron receives a gradient of $1$ from the **next layer**. We’re making up this value for demonstration purposes, and a value of 1 won’t change the values, which means that we can more easily show all of the processes. \n",
        "\n",
        "We are going to use the color of red for derivatives.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1kdRsvEPwknm-mTohSuPzUrv2S6MCYml-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10YzeN2ndbKQ"
      },
      "source": [
        "#### Sublayer - ReLU\n",
        "\n",
        "Recall that the derivative of ReLU() **with respect to its input** is 1, if the input is greater than 0, and 0 otherwise.\n",
        "\n",
        "The input value to the ReLU function is 6, so the derivative equals 1. \n",
        "\n",
        "We have to use the chain rule and multiply this derivative with the derivative received from the next layer (which we made up to be 1).\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://drive.google.com/uc?id=1ZBCFRJbpnniz3uZO5vEFC0C_xEPs7LXk)\n",
        "\n",
        "This results with the derivative of 1:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1fyBKyZ3Bz-nwGfbgRvbZGFJzojt4fyok)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: `w.r.t` stands for \"with respect to\""
      ],
      "metadata": {
        "id": "GRtcWcl3Cavn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4yGocJadzDt",
        "outputId": "1fbcb444-abc5-49a0-d2fb-bdd37e356dc6"
      },
      "source": [
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "print(deriv_relu_wrt_z)\n",
        "# -- ---------------------------------------\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q61L1oIe9Vl"
      },
      "source": [
        "#### Sublayer - Sum\n",
        "\n",
        "The partial derivative of the simple sum operation (i.e. $f(x, y, z) = x + y + z$) is always 1, no matter the inputs:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1CK16QPZ9QmMF-GQzC44ApNYF2Nm--s7m)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqHyMkFHhnDI",
        "outputId": "68eaa487-c745-4ef4-8538-df9846e316ce"
      },
      "source": [
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the sum with chain rule\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "\n",
        "# -- ----------------------\n",
        "\n",
        "print(f'deriv_relu_wrt_z  : {deriv_relu_wrt_z}')\n",
        "print(f'deriv_relu_wrt_xw0: {deriv_relu_wrt_xw0}')\n",
        "# -- ---------------------------------------\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deriv_relu_wrt_z  : 1.0\n",
            "deriv_relu_wrt_xw0: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwdNYwm5g0nP"
      },
      "source": [
        "\n",
        "![](https://drive.google.com/uc?id=1bLDvheKGVXgleqzgrIBx_tCfC-zoBxkF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2OlmAuZj93O"
      },
      "source": [
        "### For all weighted inputs and bias\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Similarity of the partial derivatives"
      ],
      "metadata": {
        "id": "d1m63T1ZGfqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note how all the partial derivatives of this \"Network\" are similar. Due to the chain rule they end up sharing a lot of the same parts.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1_XloSiT5xFdoJMLBFXjJvC8wNXbXHEK_)\n",
        "\n",
        "> For legibility, we did not denote the $\\large ReLU()$ parameter (which is the full sum), nor the $\\large sum()$ parameters (which are all of the multiplications of inputs and weights). We excluded this because the equation would be longer and harder to read. "
      ],
      "metadata": {
        "id": "5UxHjm9zE9Hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the complete one for $\\large x_0$:\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\huge \\frac{∂}{∂ x_0} = $ \n",
        "\n",
        "```python\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "# Partial derivatives of the sum, the chain rule\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "\n",
        "# Partial derivatives of the multiplication, the chain rule\n",
        "deriv_mul_wrt_x0 = w[0]\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_xw0 * deriv_mul_wrt_x0\n",
        "```"
      ],
      "metadata": {
        "id": "55xdNEuyK4jP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbQl0ODXkHJc"
      },
      "source": [
        "We can now perform the same operation.\n",
        "\n",
        "![](https://drive.google.com/uc?id=13bXiryB5dBMpa9vsAJIDVBJEHOsSO1UM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et4Fo4w5kfyE",
        "outputId": "db3c9ac7-2251-4918-a253-46c3a472c463"
      },
      "source": [
        "\n",
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# -- ----------------------\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the sum, the chain rule\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_sum_wrt_xw1 = 1\n",
        "deriv_sum_wrt_xw2 = 1\n",
        "deriv_sum_wrt_b   = 1\n",
        "\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "deriv_relu_wrt_xw1 = deriv_relu_wrt_z * deriv_sum_wrt_xw1\n",
        "deriv_relu_wrt_xw2 = deriv_relu_wrt_z * deriv_sum_wrt_xw2\n",
        "deriv_relu_wrt_b   = deriv_relu_wrt_z * deriv_sum_wrt_b\n",
        "# -- ----------------------\n",
        "\n",
        "print(f'deriv_relu_wrt_z  : {deriv_relu_wrt_z}')\n",
        "print(f'deriv_relu_wrt_xw0: {deriv_relu_wrt_xw0}')\n",
        "print(f'deriv_relu_wrt_xw1: {deriv_relu_wrt_xw1}')\n",
        "print(f'deriv_relu_wrt_xw2: {deriv_relu_wrt_xw2}')\n",
        "print(f'deriv_relu_wrt_b  : {deriv_relu_wrt_b}')\n",
        "\n",
        "# -- ---------------------------------------\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deriv_relu_wrt_z  : 1.0\n",
            "deriv_relu_wrt_xw0: 1.0\n",
            "deriv_relu_wrt_xw1: 1.0\n",
            "deriv_relu_wrt_xw2: 1.0\n",
            "deriv_relu_wrt_b  : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDfztf39k52T"
      },
      "source": [
        "### Sublayer - weights and inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAD8FKTJkLnY"
      },
      "source": [
        "Continuing backward, the next function is the multiplication of weights and inputs. \n",
        "\n",
        "The derivative for a product is whatever the input is being multiplied by. \n",
        "\n",
        "Recall:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1IAQS4eIH242ZSOLRVK3929RMBosiCx7p)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "![](https://drive.google.com/uc?id=19GdcFSnltR2iSBV2nCL7EMv_YLELhESa)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gv2hEjOlkx5",
        "outputId": "de6af10b-7881-40e9-bc77-e427b3643774"
      },
      "source": [
        "\n",
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# -- ----------------------\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the sum, the chain rule\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_sum_wrt_xw1 = 1\n",
        "deriv_sum_wrt_xw2 = 1\n",
        "deriv_sum_wrt_b   = 1\n",
        "\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "deriv_relu_wrt_xw1 = deriv_relu_wrt_z * deriv_sum_wrt_xw1\n",
        "deriv_relu_wrt_xw2 = deriv_relu_wrt_z * deriv_sum_wrt_xw2\n",
        "deriv_relu_wrt_b   = deriv_relu_wrt_z * deriv_sum_wrt_b\n",
        "# -- ----------------------\n",
        "\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the multiplication, the chain rule\n",
        "deriv_mul_wrt_x0 = w[0]\n",
        "deriv_mul_wrt_x1 = w[1]\n",
        "deriv_mul_wrt_x2 = w[2]\n",
        "\n",
        "deriv_mul_wrt_w0 = x[0]\n",
        "deriv_mul_wrt_w1 = x[1]\n",
        "deriv_mul_wrt_w2 = x[2]\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_xw0 * deriv_mul_wrt_x0\n",
        "deriv_relu_wrt_w0 = deriv_relu_wrt_xw0 * deriv_mul_wrt_w0\n",
        "\n",
        "deriv_relu_wrt_x1 = deriv_relu_wrt_xw1 * deriv_mul_wrt_x1\n",
        "deriv_relu_wrt_w1 = deriv_relu_wrt_xw1 * deriv_mul_wrt_w1\n",
        "\n",
        "deriv_relu_wrt_x2 = deriv_relu_wrt_xw2 * deriv_mul_wrt_x2\n",
        "deriv_relu_wrt_w2 = deriv_relu_wrt_xw2 * deriv_mul_wrt_w2\n",
        "# -- ----------------------\n",
        "\n",
        "\n",
        "print(f'deriv_relu_wrt_z  : {deriv_relu_wrt_z}')\n",
        "print(f'deriv_relu_wrt_xw0: {deriv_relu_wrt_xw0}')\n",
        "print(f'deriv_relu_wrt_xw1: {deriv_relu_wrt_xw1}')\n",
        "print(f'deriv_relu_wrt_xw2: {deriv_relu_wrt_xw2}')\n",
        "print(f'deriv_relu_wrt_b  : {deriv_relu_wrt_b}')\n",
        "print(f'deriv_relu_wrt_x0 : {deriv_relu_wrt_x0}')\n",
        "print(f'deriv_relu_wrt_w0 : {deriv_relu_wrt_w0}')\n",
        "print(f'deriv_relu_wrt_x1 : {deriv_relu_wrt_x1}')\n",
        "print(f'deriv_relu_wrt_w1 : {deriv_relu_wrt_w1}')\n",
        "print(f'deriv_relu_wrt_x2 : {deriv_relu_wrt_x2}')\n",
        "print(f'deriv_relu_wrt_w2 : {deriv_relu_wrt_w2}')\n",
        "\n",
        "# -- ---------------------------------------\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deriv_relu_wrt_z  : 1.0\n",
            "deriv_relu_wrt_xw0: 1.0\n",
            "deriv_relu_wrt_xw1: 1.0\n",
            "deriv_relu_wrt_xw2: 1.0\n",
            "deriv_relu_wrt_b  : 1.0\n",
            "deriv_relu_wrt_x0 : -3.0\n",
            "deriv_relu_wrt_w0 : 1.0\n",
            "deriv_relu_wrt_x1 : -1.0\n",
            "deriv_relu_wrt_w1 : -2.0\n",
            "deriv_relu_wrt_x2 : 2.0\n",
            "deriv_relu_wrt_w2 : 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the graph again: \n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=19GdcFSnltR2iSBV2nCL7EMv_YLELhESa)\n"
      ],
      "metadata": {
        "id": "p571meZcJR0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "LHeeFzoiisJz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url='https://drive.google.com/uc?id=1nSY_g3ksfY70Ui77Eg-L5mBcz4f81ruz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "4CesTGmMihpb",
        "outputId": "5bd03f63-4cd5-4150-f004-af2937c12437"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://drive.google.com/uc?id=1nSY_g3ksfY70Ui77Eg-L5mBcz4f81ruz\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bG9rBLef--V"
      },
      "source": [
        "### Simplify wrt x0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3vY-EsIiF1P"
      },
      "source": [
        "In above code, look how we applied the chain rule to calculate the partial derivative of the ReLU activation function **with respect to the first input**, $x_0$ \n",
        "\n",
        "Let’s take the related lines of the code and simplify what's needed for our final **`deriv_relu_wrt_x0`**:\n",
        "\n",
        "<br>\n",
        "\n",
        "**Original**:\n",
        "\n",
        "```python\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "\n",
        "deriv_mul_wrt_x0 = w[0]\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_xw0 * deriv_mul_wrt_x0\n",
        "```\n",
        "\n",
        "**Replace `deriv_mul_wrt_x0` with `w[0]`**:\n",
        "\n",
        "```python\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_xw0 * w[0]\n",
        "```\n",
        "\n",
        "\n",
        "**Replace `deriv_relu_wrt_xw0` with `deriv_relu_wrt_z * deriv_sum_wrt_xw0`**:\n",
        "\n",
        "```python\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0 * w[0]\n",
        "```\n",
        "\n",
        "**Replace `deriv_sum_wrt_xw0` with `1`**:\n",
        "\n",
        "```py\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_z * 1 * w[0]\n",
        "```\n",
        "\n",
        "**Replace `deriv_relu_wrt_z` with `deriv_from_next_layer * (1. if z > 0 else 0.)`**:\n",
        "\n",
        "```python\n",
        "deriv_relu_wrt_x0 = deriv_from_next_layer * (1. if z > 0 else 0.) * w[0]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run again"
      ],
      "metadata": {
        "id": "etC0NMnLO6qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B\"H\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "deriv_from_next_layer = 1.0\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_from_next_layer * (1. if z > 0 else 0.) * w[0]\n",
        "deriv_relu_wrt_x1 = deriv_from_next_layer * (1. if z > 0 else 0.) * w[1]\n",
        "deriv_relu_wrt_x2 = deriv_from_next_layer * (1. if z > 0 else 0.) * w[2]\n",
        "\n",
        "deriv_relu_wrt_w0 = deriv_from_next_layer * (1. if z > 0 else 0.) * x[0]\n",
        "deriv_relu_wrt_w1 = deriv_from_next_layer * (1. if z > 0 else 0.) * x[1]\n",
        "deriv_relu_wrt_w2 = deriv_from_next_layer * (1. if z > 0 else 0.) * x[2]\n",
        "\n",
        "deriv_relu_wrt_b  = deriv_from_next_layer * (1. if z > 0 else 0.) \n",
        "\n",
        "print(f'deriv_relu_wrt_x0 : {deriv_relu_wrt_x0}')\n",
        "print(f'deriv_relu_wrt_w0 : {deriv_relu_wrt_w0}')\n",
        "print(f'deriv_relu_wrt_x1 : {deriv_relu_wrt_x1}')\n",
        "print(f'deriv_relu_wrt_w1 : {deriv_relu_wrt_w1}')\n",
        "print(f'deriv_relu_wrt_x2 : {deriv_relu_wrt_x2}')\n",
        "print(f'deriv_relu_wrt_w2 : {deriv_relu_wrt_w2}')\n",
        "print(f'deriv_relu_wrt_b  : {deriv_relu_wrt_b}')\n",
        "# -- ---------------------------------------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFZyh1qWO7zZ",
        "outputId": "c0314bb4-713e-420e-917b-a1fdcd071d7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deriv_relu_wrt_x0 : -3.0\n",
            "deriv_relu_wrt_w0 : 1.0\n",
            "deriv_relu_wrt_x1 : -1.0\n",
            "deriv_relu_wrt_w1 : -2.0\n",
            "deriv_relu_wrt_x2 : 2.0\n",
            "deriv_relu_wrt_w2 : 3.0\n",
            "deriv_relu_wrt_b  : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the graph again: \n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=19GdcFSnltR2iSBV2nCL7EMv_YLELhESa)"
      ],
      "metadata": {
        "id": "PQ6pmo1dP3G5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUUl3vyJXF4i"
      },
      "source": [
        "### The Gradients\n",
        "\n",
        "The partial derivatives above, combined into a vector, make up our gradients. \n",
        "\n",
        "Our gradients could be represented as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSSHRJBLnk5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a98208b-af27-4b99-8f49-e8577460444e"
      },
      "source": [
        "# gradients on inputs:\n",
        "x_gradient = [deriv_relu_wrt_x0, deriv_relu_wrt_x1, deriv_relu_wrt_x2]  \n",
        "\n",
        "# gradients on weights:\n",
        "w_gradient = [deriv_relu_wrt_w0, deriv_relu_wrt_w1, deriv_relu_wrt_w2]  \n",
        "\n",
        "# gradient on bias...just 1 bias here:\n",
        "b_gradient = deriv_relu_wrt_b  \n",
        "\n",
        "print(f'x_gradient: {x_gradient}')\n",
        "print(f'w_gradient: {w_gradient}')\n",
        "print(f'b_gradient: {b_gradient}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_gradient: [-3.0, -1.0, 2.0]\n",
            "w_gradient: [1.0, -2.0, 3.0]\n",
            "b_gradient: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKPn1Ubsn_hV"
      },
      "source": [
        "> NOTE: \n",
        "> - For this single neuron example, we also won’t need our `x_gradient`. \n",
        "> - With many layers, we will continue backpropagating to preceding layers with the partial derivative with respect to our inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOmaNFaUocoj"
      },
      "source": [
        "### Apply gradients to the weights\n",
        "\n",
        "We can now apply these gradients to the weights to hopefully minimize the output. \n",
        "\n",
        "This is typically the purpose of the **optimizer** (discussed in following sections), but we can show a simplified version of this task by directly applying a negative fraction of the gradient to our weights. \n",
        "\n",
        "We apply a **negative** fraction to this gradient since we want to decrease the final output value, and the gradient shows the direction of the steepest ascent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVE0KNobo9Zd",
        "outputId": "88042fc9-ee65-453b-de26-14929bb0717c"
      },
      "source": [
        "# Our current weights and bias are:\n",
        "print(f'w: {w}')\n",
        "print(f'b: {b}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w: [-3.0, -1.0, 2.0]\n",
            "b: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NVkLG4UpPeF",
        "outputId": "55a79a0e-45c9-4cac-f990-7a5e6a724414"
      },
      "source": [
        "# Our gradients:\n",
        "print(f'w_gradient: {w_gradient}')\n",
        "print(f'b_gradient: {b_gradient}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_gradient: [1.0, -2.0, 3.0]\n",
            "b_gradient: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pN9vIKOoyB_"
      },
      "source": [
        "We can then apply a fraction of the gradients to these values:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w[0] += -0.001 * w_gradient[0]\n",
        "w[1] += -0.001 * w_gradient[1]\n",
        "w[2] += -0.001 * w_gradient[2]\n",
        "b += -0.001 * b_gradient\n",
        "\n",
        "print(w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfvEd7uHRMsh",
        "outputId": "ad47d098-c228-486f-c2ec-07fa9fc9cf72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.001, -0.998, 1.997] 0.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3rH-EDapXju"
      },
      "source": [
        "### Run another forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcCWxCutpl-z",
        "outputId": "a739fec4-cc1f-4b9b-d7cb-e45e7d243e9f"
      },
      "source": [
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "print(y)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ve successfully decreased this neuron’s output from 6.000 to 5.985.\n",
        "\n",
        "Note that it does not make sense to decrease the neuron’s output in a real neural network; we were doing this **purely as a simpler exercise than the full network**. "
      ],
      "metadata": {
        "id": "PouX8MskRZeT"
      }
    }
  ]
}