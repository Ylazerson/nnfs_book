{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part_10_01_optimizers_learning_rate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OpJdCXP_SOT"
      },
      "source": [
        "## Intro\n",
        "\n",
        "As you will soon discover, most optimizers are just variants of **Stochastic Gradient Descent (SGD)**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "753bBgOA_rH1"
      },
      "source": [
        "### SGD Naming Confusion\n",
        "\n",
        "\n",
        "You might hear the following names:\n",
        "- Stochastic Gradient Descent (SGD)\n",
        "- Vanilla Gradient Descent \n",
        "- Gradient Descent (GD)\n",
        "- Batch Gradient Descent (BGD)\n",
        "- Mini-batch Gradient Descent (MBGD)\n",
        "\n",
        "There's a long history of confusion regarding these names.\n",
        "\n",
        "<br>\n",
        " \n",
        "That said, current naming conventions in use today have merged all of these variants. \n",
        "\n",
        "Nowadays \"SGD\" assumes a batch of data; irrespective if that batch happens to be a single sample, every sample in a dataset, or some subset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Rate"
      ],
      "metadata": {
        "id": "MeApLf37wl6g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He0sUcn7ZAYH"
      },
      "source": [
        "Let’s forget, for a while, that we are performing gradient descent of an n-dimensional function (our loss function), where n is the number parameters (weights and biases) that the model contains, and assume that we have just one dimension to the loss function (a singular input). \n",
        "\n",
        "\n",
        "That said, we’ll use a real SGD optimizer on a real function for all of the following examples. \n",
        "\n",
        "---\n",
        "\n",
        "Here’s the function where we want to determine what input to it will result in the lowest possible output:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1Rw1XGCcTA5XFEq5LhstpmOvIMza9qrJH)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "We’ll start descending from the left side of this graph.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85ZbVAyiaVch"
      },
      "source": [
        "### Round 1 - LR too small\n",
        "\n",
        "![](https://drive.google.com/uc?id=1q2sI7Fg2-RUeUGvq1h7QVTMlL89BrS5h)\n",
        "\n",
        "\n",
        "The color change from green to orange to red represents the advancement of the gradient descent process, the steps. \n",
        "\n",
        "---\n",
        "\n",
        "> Note: \n",
        "> \n",
        "> How do we know if we’ve reached the global minimum or at least gotten close? \n",
        ">\n",
        "> As long as the loss value is not 0 or very close to 0, and the model stopped learning, we’re at some local minimum.  \n",
        ">\n",
        "> In reality, we almost never (and shoudn't) approach a loss of 0 for various reasons, which we'll discuss later."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "1EGVMk1hhTwA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url='https://drive.google.com/uc?id=1fKaGmrnbWXbrs_s2ZgBXNBNIV6UwcbzF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "hWg-xdmuhUKy",
        "outputId": "7667c059-102e-4b3e-8a9c-faa32a7b6262"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://drive.google.com/uc?id=1fKaGmrnbWXbrs_s2ZgBXNBNIV6UwcbzF\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6XpzOgla68c"
      },
      "source": [
        "### Round 2 - LR Still Too Small\n",
        "\n",
        "![](https://drive.google.com/uc?id=1PEll2HvrfyKNkILD7Mqe1ynKwYczqhP6)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc5pzdepbnP5"
      },
      "source": [
        "### Round 3 - Needs momentum or similar\n",
        "\n",
        "![](https://drive.google.com/uc?id=1e1tfv8FXI4maDQdYGn_ZExDEmu_EymFX)\n",
        "\n",
        "The model was able to escape the “deeper” local minimums, so it might be counter-intuitive why it is stuck here. \n",
        "\n",
        "Remember, the model follows the direction of steepest descent of the loss function, no matter how large or slight the descent is. For this reason, we’ll introduce **momentum** and the other techniques to prevent such situations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyXnlPvEbozH"
      },
      "source": [
        "### Momentum\n",
        "\n",
        "Momentum, in an optimizer, adds to the gradient what, in the physical world, we could call **inertia** — for example, we can throw a ball uphill and, with a small enough hill or big enough applied force, the ball can roll-over to the other side of the hill."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS7hKFuCcdgY"
      },
      "source": [
        "### Round 4 - Large momentum yet LR too small\n",
        "\n",
        "![](https://drive.google.com/uc?id=1UYwJJa9c4o3XHNCGwvaeYAmk3UppwX66)\n",
        "\n",
        "We used a very small learning rate here with a large momentum. \n",
        "\n",
        "We can see that the model achieved the goal and found the global minimum, but this took many steps. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7-NyuJpdFxB"
      },
      "source": [
        "### Round 5 - Good momentum and LR\n",
        "\n",
        "![](https://drive.google.com/uc?id=1GHIp3LJa77q9gWs2gzqqHBAJ7BYg-WTP)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url='https://drive.google.com/uc?id=1kTuvrNg4fGGxDNLdlu4puyBxkP7F8A-5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "ulsG_EHhkiix",
        "outputId": "5fdf07fa-76a9-4e11-f99d-8298f968dd9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://drive.google.com/uc?id=1kTuvrNg4fGGxDNLdlu4puyBxkP7F8A-5\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we were able to find the global minimum in about 50 steps.  \n",
        "\n",
        "It’s possible to significantly shorten the training time by adjusting the parameters of the optimizer."
      ],
      "metadata": {
        "id": "DFbVMY_YkldJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj1dESROdaS7"
      },
      "source": [
        "### Round 6 - LR too high\n",
        "\n",
        "![](https://drive.google.com/uc?id=1AJaCdGKdUT7nluurFr7gWqdpuknhW5-J)\n",
        "\n",
        "With the learning rate set too high, the model might not be able to find the global minimum. \n",
        "\n",
        "Even, at some point, if it does, further adjustments could cause it to **jump out** of this minimum. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axqWomoId5RU"
      },
      "source": [
        "### Round 7 - LR way too high\n",
        "\n",
        "![](https://drive.google.com/uc?id=13PHWk5BWboLjEQjTD9a3mnrx4i0M73q9)\n",
        "\n",
        "In this extreme situation we have a **gradient explosion**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Q7vFLLee0M"
      },
      "source": [
        "### Gradient Explosion\n",
        "\n",
        "A gradient explosion is a situation where the parameter updates cause the function’s output to **rise** instead of fall, and, with each step, the loss value and gradient become larger. \n",
        "\n",
        "At some point, the floating-point variable limitation causes an overflow as it cannot hold values of this size anymore, and the model is no longer able to train. \n",
        "\n",
        "<br>\n",
        "\n",
        "It’s crucial to recognize this situation forming during training, especially for large models, where the training can take days, weeks, or more. It is possible to tune the model’s hyper-parameters in time to save the model and to continue training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjGOGfsIbo1f"
      },
      "source": [
        "### Round 8 - Great hyperparameters\n",
        "\n",
        "![](https://drive.google.com/uc?id=1rz2F8zWH_yySJkjyQBqrYq-RHsn0S4_Z)\n",
        "\n",
        "This time the model needed just a few steps to find the global minimum. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKYklVI_fccQ"
      },
      "source": [
        "### Summary\n",
        "\n",
        "It is a challenge to choose the hyper-parameters correctly, and it is not always an easy task. \n",
        "\n",
        "It is usually best to start with the optimizer defaults, perform a few steps, and observe the training process when tuning different settings. \n",
        "\n",
        "It is not always possible to see meaningful results in a short-enough period of time, and, in this case, it’s good to have the ability to update the optimizer’s settings during training. \n",
        "\n",
        "---\n",
        "\n",
        "For a summary of learning rates — if we plot the loss along an axis of steps:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1zFBAb4vKe6JoXr36Rcsk_0c0-_HcApp0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXkDhTDdhDm4"
      },
      "source": [
        "## Learning Rate Decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX_i42PQhIXL"
      },
      "source": [
        "### Idea\n",
        "\n",
        "The idea of a learning rate decay is to **start with a large learning rate**, say 1.0 in our case, and **then decrease** it during training. \n",
        "\n",
        "The model needs **small updates near the end** of training to be able to get as close to the minimum point as possible.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "There are a few methods for doing this:\n",
        "- One is to decrease the learning rate in response to the loss across epochs.  \n",
        "    - For example, if the loss begins to level out/plateau or starts “jumping” over large deltas. \n",
        "    - You can either program this behavior-monitoring logically or simply track your loss over time and manually decrease the learning rate when you deem it appropriate. \n",
        "- Another option, **which we will implement**, is to program a **Decay _Rate_**, which **steadily decays** the learning rate per batch or epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBMZSj5shOnx"
      },
      "source": [
        "### Our Decay Rate\n",
        "\n",
        "Let’s plan to **decay per step**. \n",
        "\n",
        "This can also be referred to as **1/t decaying** or **exponential decaying**. \n",
        "\n",
        "<br>\n",
        "\n",
        "<u>**Details**</u>\n",
        "\n",
        "We’re going to **update the learning rate each step** by the **reciprocal of the step count fraction**. \n",
        "\n",
        "This **fraction** is a new hyper-parameter that we’ll add to the optimizer, called the **learning rate decay**. \n",
        "\n",
        "<br>\n",
        "\n",
        "$\\large rate = startingRate * ( \\frac {1}  {1 + rateDecay * step})$\n",
        "\n",
        "The added 1 makes sure that the resulting algorithm never raises the learning rate. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be20uHUaiwKM"
      },
      "source": [
        "### Example\n",
        "\n",
        "Note that in practice, 0.1 would be considered a fairly aggressive decay rate, but this should give you a sense of the concept.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boq6Oporitcy",
        "outputId": "8ae7ae54-a3d8-4932-c5aa-0135cd898c72"
      },
      "source": [
        "starting_learning_rate = 1.\n",
        "learning_rate_decay = 0.1\n",
        "\n",
        "prev_learning_rate = starting_learning_rate\n",
        "\n",
        "for step in range(30):\n",
        "    \n",
        "    learning_rate = starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
        "    \n",
        "    # -- --------------------------------\n",
        "    diff_from_prev = prev_learning_rate - learning_rate\n",
        "    print(f'step: {step:02}, learning rate: {learning_rate:.4f}, diff from prev: {diff_from_prev:.4f}')\n",
        "\n",
        "    prev_learning_rate = learning_rate"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 00, learning rate: 1.0000, diff from prev: 0.0000\n",
            "step: 01, learning rate: 0.9091, diff from prev: 0.0909\n",
            "step: 02, learning rate: 0.8333, diff from prev: 0.0758\n",
            "step: 03, learning rate: 0.7692, diff from prev: 0.0641\n",
            "step: 04, learning rate: 0.7143, diff from prev: 0.0549\n",
            "step: 05, learning rate: 0.6667, diff from prev: 0.0476\n",
            "step: 06, learning rate: 0.6250, diff from prev: 0.0417\n",
            "step: 07, learning rate: 0.5882, diff from prev: 0.0368\n",
            "step: 08, learning rate: 0.5556, diff from prev: 0.0327\n",
            "step: 09, learning rate: 0.5263, diff from prev: 0.0292\n",
            "step: 10, learning rate: 0.5000, diff from prev: 0.0263\n",
            "step: 11, learning rate: 0.4762, diff from prev: 0.0238\n",
            "step: 12, learning rate: 0.4545, diff from prev: 0.0216\n",
            "step: 13, learning rate: 0.4348, diff from prev: 0.0198\n",
            "step: 14, learning rate: 0.4167, diff from prev: 0.0181\n",
            "step: 15, learning rate: 0.4000, diff from prev: 0.0167\n",
            "step: 16, learning rate: 0.3846, diff from prev: 0.0154\n",
            "step: 17, learning rate: 0.3704, diff from prev: 0.0142\n",
            "step: 18, learning rate: 0.3571, diff from prev: 0.0132\n",
            "step: 19, learning rate: 0.3448, diff from prev: 0.0123\n",
            "step: 20, learning rate: 0.3333, diff from prev: 0.0115\n",
            "step: 21, learning rate: 0.3226, diff from prev: 0.0108\n",
            "step: 22, learning rate: 0.3125, diff from prev: 0.0101\n",
            "step: 23, learning rate: 0.3030, diff from prev: 0.0095\n",
            "step: 24, learning rate: 0.2941, diff from prev: 0.0089\n",
            "step: 25, learning rate: 0.2857, diff from prev: 0.0084\n",
            "step: 26, learning rate: 0.2778, diff from prev: 0.0079\n",
            "step: 27, learning rate: 0.2703, diff from prev: 0.0075\n",
            "step: 28, learning rate: 0.2632, diff from prev: 0.0071\n",
            "step: 29, learning rate: 0.2564, diff from prev: 0.0067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXCkz-Vln0zg"
      },
      "source": [
        "## Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMVxXKEn1qe"
      },
      "source": [
        "### Intro\n",
        "\n",
        "Stochastic Gradient Descent with **learning rate decay** can do fairly well but is still a fairly basic optimization method.\n",
        "\n",
        "One option for improving the SGD optimizer is to introduce **momentum**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxs4EYG4oFN9"
      },
      "source": [
        "### Idea\n",
        "\n",
        "Momentum creates a **rolling average of gradients** over some number of updates and uses this average with the unique gradient at each step. \n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "Another way of understanding this is to imagine a ball going down a hill — even if it finds a small hole or hill, momentum will let it go straight through it towards a lower minimum — the bottom of this hill. This can help in cases where you’re stuck in some local minimum (a hole), bouncing back and forth. With momentum, a model is more likely to pass through local minimums, further decreasing loss. \n",
        "\n",
        "Simply put, **momentum may still point towards the global gradient descent direction**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "With regular updates, the SGD optimizer might determine that the **next best step** is one that keeps the model in a **local minimum**. \n",
        "\n",
        "The step may decrease loss for that update but might not get us out of the local minimum. We might wind up with a gradient that points in one direction and then the opposite direction in the next update; the gradient could continue to bounce back and forth around a local minimum like this, keeping the optimization of the loss stuck.\n",
        "\n",
        "Instead, **momentum uses the previous update’s direction to influence the next update’s direction**, minimizing the chances of bouncing around and getting stuck.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "We utilize momentum by setting a parameter between 0 and 1, representing the fraction of the previous parameter update to retain.\n",
        "\n",
        "The update contains a portion of the gradient from preceding steps as our momentum (direction of previous changes) and only a portion of the current gradient; together, these portions form the actual change to our parameters and **the bigger the role that momentum takes in the update, the slower the update can change the direction**. \n",
        "\n",
        "When we set the momentum fraction too high, the model might stop learning at all since the direction of the updates won’t be able to follow the global gradient descent. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxF8Bg01tRV8"
      },
      "source": [
        "### Takeaway\n",
        "\n",
        "The SGD optimizer with momentum is usually one of 2 main choices for an optimizer in practice next to the Adam optimizer, which we’ll talk about shortly. \n",
        "\n",
        "For example, see the **Scenario 5** in the following notebook where use SGD with:\n",
        "- learning rate of `1` \n",
        "- decay rate of `1e-3` \n",
        "- momentum of `0.9`\n",
        "\n",
        "... here's the result:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url='https://drive.google.com/uc?id=1juRqu1O3niaC8iAPT1meLnaMUBYK0uQg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "gQ6gueB4rNXf",
        "outputId": "99387732-42eb-434d-efaf-4e51bec82a26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://drive.google.com/uc?id=1juRqu1O3niaC8iAPT1meLnaMUBYK0uQg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to understand the above animation:\n",
        "- The colorful squares on the right show weights and biases — red for positive and blue for negative values. \n",
        "- The matching areas right below the Dense 1 bar and next to the Dense 2 bar show the updates that the optimizer performs to the layers. \n",
        "- The updates might look overly strong compared to the weights and biases, but that’s because we’ve visually normalized them to the maximum value, or else they would be almost invisible since the updates are quite small at a time."
      ],
      "metadata": {
        "id": "vvmwDwlorNfs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMvNCXVnyva8"
      },
      "source": [
        "## AdaGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTbEbhiX0m1V"
      },
      "source": [
        "**NOTE: ** this optimizer is **not** widely used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pG-9RodyxR-"
      },
      "source": [
        "### Idea\n",
        "\n",
        "**AdaGrad**, short for **adaptive gradient**, institutes a **per-parameter learning rate** rather than a **globally-shared rate** (as we did before). \n",
        "\n",
        "Overall, the impact is the learning rates for parameters with smaller gradients are decreased slowly, while the parameters with larger gradients have their learning rates decreased faster.\n",
        "\n",
        "<br>\n",
        "\n",
        "<u>Details</u>\n",
        "\n",
        "During the training process, some weights can rise significantly, while others tend to not change by much. It is usually better for weights to not rise too high compared to the other weights (we’ll talk about this with regularization techniques in later chapters). \n",
        "\n",
        "AdaGrad provides a way to normalize parameter updates by keeping a history of previous updates — the bigger the sum of the updates is, in either direction (positive or negative), the smaller updates are made further in training. \n",
        "\n",
        "This lets less-frequently updated parameters to keep-up with changes, effectively utilizing more neurons for training. \n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "We won't go into futher details here. However, keep in mind that the AdaGrad approach sometimes causes the learning to stall as updates become smaller with time. \n",
        "\n",
        "This is one major reason why this optimizer is not widely used, except for some specific applications. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIg014hy1saO"
      },
      "source": [
        "## RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjjwOA6H22O5"
      },
      "source": [
        "### Idea\n",
        "\n",
        "**RMSProp** is short for **Root Mean Square Propagation**. \n",
        "\n",
        "Similar to AdaGrad, RMSProp calculates an adaptive learning rate per parameter; it’s just calculated in a different way than AdaGrad.\n",
        "\n",
        "\n",
        "Again, its goal is to **retain the global direction of changes and slows changes in direction**. \n",
        "\n",
        "\n",
        "We won't go into futher details here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnSOVEf54eNb"
      },
      "source": [
        "## Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES-MrmzJ60-E"
      },
      "source": [
        "### Idea\n",
        "\n",
        "**Adam**, short for **Adaptive Momentum**, is currently the **most widely-used optimizer**\n",
        "\n",
        "It is built atop RMSProp, with the momentum concept from SGD added back in. \n",
        "\n",
        "At a high-level; instead of applying current gradients, iy applies momentums (like we did for SGD), then it applies a per-weight adaptive learning rate (as in RMSProp).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaOFWK067cKF"
      },
      "source": [
        "### Note on Bias Correction Mechanism\n",
        "\n",
        "Do not confuse this with the layer’s bias. \n",
        "\n",
        "This mechanism in Adam compensates for the initial zeroed values before they warm up in the initial steps. \n",
        "\n",
        "This mechanism significantly speeds up training in the initial stages before its finally \"warmed up\" after many steps. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JpDH-ri22n1"
      },
      "source": [
        "## Summary of Optimizers\n",
        "\n",
        "**Adam** is usually the best optimizer.\n",
        "\n",
        "That’s not always the case though. It’s usually a good idea to try the Adam optimizer first but to also try the others, especially if you’re not getting the results you hoped for. Sometimes simple **SGD** or **SGD + momentum** performs better than Adam. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwnAeMdGfLeH"
      },
      "source": [
        "## How to choose hyperparams\n",
        "\n",
        "It is not always an easy task. \n",
        "\n",
        "It is usually best to start with the optimizer **defaults**, perform a few steps, and observe the training process when tuning different settings. \n",
        "\n",
        "---\n",
        "\n",
        "For **SGD**'s **learning rate**, a good rule is that your initial training will benefit from a **larger** learning rate to take initial steps faster. If you start with steps that are too small, you might get stuck in a local minimum and be unable to leave it due to not making large enough updates to the parameters.\n",
        "\n",
        "---\n",
        "\n",
        "There is no single, best way to set hyper-parameters, but experience usually helps :)\n",
        "\n",
        "\n",
        "---\n",
        "**General Guidelines on LR**\n",
        "\n",
        "- Starting LR for SGD is 1.0, with a decay down to 0.1. \n",
        "- For Adam, a good starting LR is 0.001 (1e-3), decaying down to 0.0001 (1e-4). \n"
      ]
    }
  ]
}