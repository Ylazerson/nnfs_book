{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_full_code_with_various_runs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o94vZ8_rAzm9"
      },
      "source": [
        "# B\"H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rARLYSfqA05f"
      },
      "source": [
        "## Setup "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqwhEdVkBGWz",
        "outputId": "cd2e7b2b-91db-4fd6-d39d-bc529dcc3321"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQKCu7oeBFWU"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij-P9rN1BJo8"
      },
      "source": [
        "## Network Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMEDz8ifBUmz"
      },
      "source": [
        "### Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5dvPcY9A_Qr"
      },
      "source": [
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IlpQE39BbOe"
      },
      "source": [
        "### ReLU activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfvCES-oBfHt"
      },
      "source": [
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joHLFaQ0BhQT"
      },
      "source": [
        "### Softmax activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0dcSopHBl6G"
      },
      "source": [
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B426Sbr1A_St"
      },
      "source": [
        "### SGD optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCdpx57xBsCg"
      },
      "source": [
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1QODKqA_U4"
      },
      "source": [
        "### Adagrad optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuONna6iBzPW"
      },
      "source": [
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights**2\n",
        "        layer.bias_cache += layer.dbiases**2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tyq5dF_B5vw"
      },
      "source": [
        "### RMSprop optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnTxmYGpB7Lv"
      },
      "source": [
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "            (1 - self.rho) * layer.dweights**2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "            (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieZ1jGFgCB2c"
      },
      "source": [
        "### Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJqAst_-CCka"
      },
      "source": [
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "            (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "            (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "            (1 - self.beta_2) * layer.dweights**2\n",
        "\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "            (1 - self.beta_2) * layer.dbiases**2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                             self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                         bias_momentums_corrected / \\\n",
        "                         (np.sqrt(bias_cache_corrected) +\n",
        "                             self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiOKXAgyCOe4"
      },
      "source": [
        "### Common loss class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKKgwNodCPLz"
      },
      "source": [
        "class Loss:\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbrp7tj2CRuj"
      },
      "source": [
        "### Cross-entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bLCNcEiCSfS"
      },
      "source": [
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGRzhIbeCZMx"
      },
      "source": [
        "### Combined Softmax activation and cross-entropy loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CENYbocgCZ2q"
      },
      "source": [
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKxauDleHT6R"
      },
      "source": [
        "## Scenario 1 - Vanilla SGD (LR 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf_-nSH6HT6R"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBl-5qV6HT6R"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rABbMGKNHT6R"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-EFx1cmHdY6"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output \n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv_wBUbNHT6S"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Remember, each full pass through all of the training data is called an **epoch**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J5OcH5IH-wC",
        "outputId": "0f806e9f-a7a3-419e-ba40-73cedbbc8919"
      },
      "source": [
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "\n",
        "    # -- -------------------------------\n",
        "    # Perform a forward pass of our training data through this layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Perform a forward pass through activation function\n",
        "    # takes the output of first dense layer here\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Perform a forward pass through second Dense layer\n",
        "    # takes outputs of activation function of first layer as inputs\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Perform a forward pass through the activation/loss function\n",
        "    # takes the output of second dense layer here and returns loss\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # -- -------------------------------\n",
        "    # Calculate accuracy from output of activation2 and targets\n",
        "    # calculate values along first axis\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions==y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}')\n",
        "    \n",
        "    # -- -------------------------------\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.263, loss: 1.099\n",
            "epoch: 100, acc: 0.420, loss: 1.075\n",
            "epoch: 200, acc: 0.440, loss: 1.062\n",
            "epoch: 300, acc: 0.447, loss: 1.059\n",
            "epoch: 400, acc: 0.453, loss: 1.057\n",
            "epoch: 500, acc: 0.450, loss: 1.055\n",
            "epoch: 600, acc: 0.453, loss: 1.052\n",
            "epoch: 700, acc: 0.460, loss: 1.047\n",
            "epoch: 800, acc: 0.453, loss: 1.050\n",
            "epoch: 900, acc: 0.453, loss: 1.047\n",
            "epoch: 1000, acc: 0.437, loss: 1.051\n",
            "epoch: 1100, acc: 0.447, loss: 1.041\n",
            "epoch: 1200, acc: 0.453, loss: 1.034\n",
            "epoch: 1300, acc: 0.463, loss: 1.023\n",
            "epoch: 1400, acc: 0.460, loss: 1.013\n",
            "epoch: 1500, acc: 0.450, loss: 1.004\n",
            "epoch: 1600, acc: 0.437, loss: 0.991\n",
            "epoch: 1700, acc: 0.440, loss: 0.974\n",
            "epoch: 1800, acc: 0.517, loss: 0.959\n",
            "epoch: 1900, acc: 0.473, loss: 0.978\n",
            "epoch: 2000, acc: 0.497, loss: 0.948\n",
            "epoch: 2100, acc: 0.503, loss: 0.920\n",
            "epoch: 2200, acc: 0.507, loss: 0.925\n",
            "epoch: 2300, acc: 0.587, loss: 0.887\n",
            "epoch: 2400, acc: 0.540, loss: 0.868\n",
            "epoch: 2500, acc: 0.613, loss: 0.849\n",
            "epoch: 2600, acc: 0.593, loss: 0.824\n",
            "epoch: 2700, acc: 0.580, loss: 0.823\n",
            "epoch: 2800, acc: 0.603, loss: 0.816\n",
            "epoch: 2900, acc: 0.600, loss: 0.790\n",
            "epoch: 3000, acc: 0.603, loss: 0.815\n",
            "epoch: 3100, acc: 0.643, loss: 0.725\n",
            "epoch: 3200, acc: 0.547, loss: 0.873\n",
            "epoch: 3300, acc: 0.667, loss: 0.694\n",
            "epoch: 3400, acc: 0.667, loss: 0.686\n",
            "epoch: 3500, acc: 0.640, loss: 0.742\n",
            "epoch: 3600, acc: 0.653, loss: 0.684\n",
            "epoch: 3700, acc: 0.623, loss: 0.837\n",
            "epoch: 3800, acc: 0.700, loss: 0.677\n",
            "epoch: 3900, acc: 0.693, loss: 0.663\n",
            "epoch: 4000, acc: 0.670, loss: 0.729\n",
            "epoch: 4100, acc: 0.697, loss: 0.628\n",
            "epoch: 4200, acc: 0.687, loss: 0.623\n",
            "epoch: 4300, acc: 0.693, loss: 0.631\n",
            "epoch: 4400, acc: 0.713, loss: 0.630\n",
            "epoch: 4500, acc: 0.750, loss: 0.612\n",
            "epoch: 4600, acc: 0.670, loss: 0.675\n",
            "epoch: 4700, acc: 0.713, loss: 0.588\n",
            "epoch: 4800, acc: 0.717, loss: 0.593\n",
            "epoch: 4900, acc: 0.723, loss: 0.596\n",
            "epoch: 5000, acc: 0.677, loss: 0.654\n",
            "epoch: 5100, acc: 0.723, loss: 0.577\n",
            "epoch: 5200, acc: 0.713, loss: 0.591\n",
            "epoch: 5300, acc: 0.703, loss: 0.636\n",
            "epoch: 5400, acc: 0.693, loss: 0.687\n",
            "epoch: 5500, acc: 0.760, loss: 0.570\n",
            "epoch: 5600, acc: 0.740, loss: 0.579\n",
            "epoch: 5700, acc: 0.730, loss: 0.571\n",
            "epoch: 5800, acc: 0.740, loss: 0.559\n",
            "epoch: 5900, acc: 0.743, loss: 0.559\n",
            "epoch: 6000, acc: 0.740, loss: 0.561\n",
            "epoch: 6100, acc: 0.753, loss: 0.563\n",
            "epoch: 6200, acc: 0.750, loss: 0.572\n",
            "epoch: 6300, acc: 0.747, loss: 0.584\n",
            "epoch: 6400, acc: 0.773, loss: 0.505\n",
            "epoch: 6500, acc: 0.653, loss: 0.839\n",
            "epoch: 6600, acc: 0.757, loss: 0.565\n",
            "epoch: 6700, acc: 0.767, loss: 0.567\n",
            "epoch: 6800, acc: 0.773, loss: 0.543\n",
            "epoch: 6900, acc: 0.773, loss: 0.544\n",
            "epoch: 7000, acc: 0.773, loss: 0.550\n",
            "epoch: 7100, acc: 0.773, loss: 0.548\n",
            "epoch: 7200, acc: 0.763, loss: 0.548\n",
            "epoch: 7300, acc: 0.770, loss: 0.539\n",
            "epoch: 7400, acc: 0.600, loss: 1.044\n",
            "epoch: 7500, acc: 0.653, loss: 0.803\n",
            "epoch: 7600, acc: 0.637, loss: 0.852\n",
            "epoch: 7700, acc: 0.673, loss: 0.751\n",
            "epoch: 7800, acc: 0.713, loss: 0.662\n",
            "epoch: 7900, acc: 0.807, loss: 0.486\n",
            "epoch: 8000, acc: 0.780, loss: 0.511\n",
            "epoch: 8100, acc: 0.803, loss: 0.486\n",
            "epoch: 8200, acc: 0.793, loss: 0.490\n",
            "epoch: 8300, acc: 0.787, loss: 0.488\n",
            "epoch: 8400, acc: 0.790, loss: 0.488\n",
            "epoch: 8500, acc: 0.793, loss: 0.483\n",
            "epoch: 8600, acc: 0.797, loss: 0.484\n",
            "epoch: 8700, acc: 0.793, loss: 0.485\n",
            "epoch: 8800, acc: 0.757, loss: 0.544\n",
            "epoch: 8900, acc: 0.807, loss: 0.467\n",
            "epoch: 9000, acc: 0.810, loss: 0.475\n",
            "epoch: 9100, acc: 0.647, loss: 0.917\n",
            "epoch: 9200, acc: 0.820, loss: 0.456\n",
            "epoch: 9300, acc: 0.800, loss: 0.484\n",
            "epoch: 9400, acc: 0.817, loss: 0.452\n",
            "epoch: 9500, acc: 0.823, loss: 0.452\n",
            "epoch: 9600, acc: 0.817, loss: 0.451\n",
            "epoch: 9700, acc: 0.823, loss: 0.444\n",
            "epoch: 9800, acc: 0.820, loss: 0.447\n",
            "epoch: 9900, acc: 0.833, loss: 0.441\n",
            "epoch: 10000, acc: 0.827, loss: 0.440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVzfgTNMgL1R"
      },
      "source": [
        "## Scenario 2 - SGD (LR 0.85)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y_1HaY7gL1S"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIx-dtnLgL1S"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONlj-VeVgL1S"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHldF1sigL1S"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output \n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD(learning_rate=.85)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2sVd3x_gL1S"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Remember, each full pass through all of the training data is called an **epoch**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_s726KugL1S",
        "outputId": "38607afb-c17a-4e3e-c31f-f26b57af192a"
      },
      "source": [
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "\n",
        "    # -- -------------------------------\n",
        "    # Perform a forward pass of our training data through this layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Perform a forward pass through activation function\n",
        "    # takes the output of first dense layer here\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Perform a forward pass through second Dense layer\n",
        "    # takes outputs of activation function of first layer as inputs\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Perform a forward pass through the activation/loss function\n",
        "    # takes the output of second dense layer here and returns loss\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # -- -------------------------------\n",
        "    # Calculate accuracy from output of activation2 and targets\n",
        "    # calculate values along first axis\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions==y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}')\n",
        "    \n",
        "    # -- -------------------------------\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.290, loss: 1.099\n",
            "epoch: 100, acc: 0.413, loss: 1.064\n",
            "epoch: 200, acc: 0.443, loss: 1.060\n",
            "epoch: 300, acc: 0.447, loss: 1.057\n",
            "epoch: 400, acc: 0.460, loss: 1.054\n",
            "epoch: 500, acc: 0.473, loss: 1.050\n",
            "epoch: 600, acc: 0.487, loss: 1.047\n",
            "epoch: 700, acc: 0.493, loss: 1.042\n",
            "epoch: 800, acc: 0.473, loss: 1.035\n",
            "epoch: 900, acc: 0.403, loss: 1.037\n",
            "epoch: 1000, acc: 0.393, loss: 1.033\n",
            "epoch: 1100, acc: 0.397, loss: 1.025\n",
            "epoch: 1200, acc: 0.407, loss: 1.015\n",
            "epoch: 1300, acc: 0.460, loss: 1.020\n",
            "epoch: 1400, acc: 0.463, loss: 1.001\n",
            "epoch: 1500, acc: 0.497, loss: 1.007\n",
            "epoch: 1600, acc: 0.443, loss: 1.008\n",
            "epoch: 1700, acc: 0.447, loss: 0.981\n",
            "epoch: 1800, acc: 0.443, loss: 0.971\n",
            "epoch: 1900, acc: 0.457, loss: 0.964\n",
            "epoch: 2000, acc: 0.480, loss: 0.959\n",
            "epoch: 2100, acc: 0.473, loss: 0.968\n",
            "epoch: 2200, acc: 0.467, loss: 0.959\n",
            "epoch: 2300, acc: 0.503, loss: 0.961\n",
            "epoch: 2400, acc: 0.497, loss: 0.957\n",
            "epoch: 2500, acc: 0.467, loss: 0.971\n",
            "epoch: 2600, acc: 0.490, loss: 0.952\n",
            "epoch: 2700, acc: 0.517, loss: 0.952\n",
            "epoch: 2800, acc: 0.490, loss: 0.959\n",
            "epoch: 2900, acc: 0.493, loss: 0.954\n",
            "epoch: 3000, acc: 0.520, loss: 0.949\n",
            "epoch: 3100, acc: 0.497, loss: 0.960\n",
            "epoch: 3200, acc: 0.520, loss: 0.934\n",
            "epoch: 3300, acc: 0.497, loss: 0.951\n",
            "epoch: 3400, acc: 0.503, loss: 0.953\n",
            "epoch: 3500, acc: 0.523, loss: 0.945\n",
            "epoch: 3600, acc: 0.493, loss: 0.943\n",
            "epoch: 3700, acc: 0.513, loss: 0.962\n",
            "epoch: 3800, acc: 0.480, loss: 0.951\n",
            "epoch: 3900, acc: 0.490, loss: 0.942\n",
            "epoch: 4000, acc: 0.513, loss: 0.961\n",
            "epoch: 4100, acc: 0.473, loss: 0.944\n",
            "epoch: 4200, acc: 0.533, loss: 0.939\n",
            "epoch: 4300, acc: 0.477, loss: 0.912\n",
            "epoch: 4400, acc: 0.560, loss: 0.907\n",
            "epoch: 4500, acc: 0.503, loss: 0.910\n",
            "epoch: 4600, acc: 0.517, loss: 0.881\n",
            "epoch: 4700, acc: 0.557, loss: 0.899\n",
            "epoch: 4800, acc: 0.553, loss: 0.879\n",
            "epoch: 4900, acc: 0.587, loss: 0.866\n",
            "epoch: 5000, acc: 0.587, loss: 0.832\n",
            "epoch: 5100, acc: 0.577, loss: 0.843\n",
            "epoch: 5200, acc: 0.550, loss: 0.896\n",
            "epoch: 5300, acc: 0.590, loss: 0.812\n",
            "epoch: 5400, acc: 0.590, loss: 0.835\n",
            "epoch: 5500, acc: 0.560, loss: 0.840\n",
            "epoch: 5600, acc: 0.637, loss: 0.776\n",
            "epoch: 5700, acc: 0.587, loss: 0.809\n",
            "epoch: 5800, acc: 0.630, loss: 0.795\n",
            "epoch: 5900, acc: 0.590, loss: 0.809\n",
            "epoch: 6000, acc: 0.627, loss: 0.776\n",
            "epoch: 6100, acc: 0.593, loss: 0.812\n",
            "epoch: 6200, acc: 0.587, loss: 0.791\n",
            "epoch: 6300, acc: 0.597, loss: 0.780\n",
            "epoch: 6400, acc: 0.593, loss: 0.809\n",
            "epoch: 6500, acc: 0.620, loss: 0.779\n",
            "epoch: 6600, acc: 0.550, loss: 0.856\n",
            "epoch: 6700, acc: 0.587, loss: 0.770\n",
            "epoch: 6800, acc: 0.580, loss: 0.829\n",
            "epoch: 6900, acc: 0.623, loss: 0.789\n",
            "epoch: 7000, acc: 0.557, loss: 0.853\n",
            "epoch: 7100, acc: 0.593, loss: 0.761\n",
            "epoch: 7200, acc: 0.603, loss: 0.747\n",
            "epoch: 7300, acc: 0.643, loss: 0.768\n",
            "epoch: 7400, acc: 0.573, loss: 0.867\n",
            "epoch: 7500, acc: 0.637, loss: 0.765\n",
            "epoch: 7600, acc: 0.483, loss: 1.115\n",
            "epoch: 7700, acc: 0.677, loss: 0.751\n",
            "epoch: 7800, acc: 0.637, loss: 0.756\n",
            "epoch: 7900, acc: 0.613, loss: 0.782\n",
            "epoch: 8000, acc: 0.597, loss: 0.812\n",
            "epoch: 8100, acc: 0.697, loss: 0.670\n",
            "epoch: 8200, acc: 0.590, loss: 0.771\n",
            "epoch: 8300, acc: 0.663, loss: 0.838\n",
            "epoch: 8400, acc: 0.690, loss: 0.775\n",
            "epoch: 8500, acc: 0.617, loss: 0.820\n",
            "epoch: 8600, acc: 0.630, loss: 0.748\n",
            "epoch: 8700, acc: 0.653, loss: 0.736\n",
            "epoch: 8800, acc: 0.637, loss: 0.780\n",
            "epoch: 8900, acc: 0.667, loss: 0.722\n",
            "epoch: 9000, acc: 0.637, loss: 0.741\n",
            "epoch: 9100, acc: 0.627, loss: 0.735\n",
            "epoch: 9200, acc: 0.537, loss: 1.034\n",
            "epoch: 9300, acc: 0.717, loss: 0.742\n",
            "epoch: 9400, acc: 0.633, loss: 0.753\n",
            "epoch: 9500, acc: 0.633, loss: 0.684\n",
            "epoch: 9600, acc: 0.607, loss: 0.702\n",
            "epoch: 9700, acc: 0.667, loss: 0.721\n",
            "epoch: 9800, acc: 0.710, loss: 0.709\n",
            "epoch: 9900, acc: 0.613, loss: 0.928\n",
            "epoch: 10000, acc: 0.697, loss: 0.623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELEcM3HOl3VR"
      },
      "source": [
        "## Scenario 3 - SGD (LR 1, decay=1e-2)\n",
        "\n",
        "[Vid](https://www.youtube.com/watch?v=TgP-i5A1sjY)\n",
        "\n",
        "This model definitely got stuck, and the reason is almost certainly because the **learning rate decayed far too quickly** and became too small, **trapping the model in some local minimum**. \n",
        "\n",
        "This is most likely why, rather than wiggling, our accuracy and loss stopped changing at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXlrjAKVl3VR"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PxT0nTgl3VR"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etacEwKjl3VR"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5gzNq4tl3VR"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output \n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD(decay=1e-2)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgSnKKg9l3VR"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Remember, each full pass through all of the training data is called an **epoch**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "begU0nctmNL8",
        "outputId": "6a2d4cc7-140e-4ab5-c8fe-b7cb649f7047"
      },
      "source": [
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "\n",
        "    # Perform a forward pass of our training data through this layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Perform a forward pass through activation function\n",
        "    # takes the output of first dense layer here\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Perform a forward pass through second Dense layer\n",
        "    # takes outputs of activation function of first layer as inputs\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Perform a forward pass through the activation/loss function\n",
        "    # takes the output of second dense layer here and returns loss\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy from output of activation2 and targets\n",
        "    # calculate values along first axis\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions==y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "    \n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.293, loss: 1.099, lr: 1.0\n",
            "epoch: 100, acc: 0.417, loss: 1.084, lr: 0.5025125628140703\n",
            "epoch: 200, acc: 0.447, loss: 1.066, lr: 0.33444816053511706\n",
            "epoch: 300, acc: 0.443, loss: 1.063, lr: 0.2506265664160401\n",
            "epoch: 400, acc: 0.447, loss: 1.061, lr: 0.2004008016032064\n",
            "epoch: 500, acc: 0.457, loss: 1.060, lr: 0.1669449081803005\n",
            "epoch: 600, acc: 0.457, loss: 1.060, lr: 0.14306151645207438\n",
            "epoch: 700, acc: 0.453, loss: 1.060, lr: 0.1251564455569462\n",
            "epoch: 800, acc: 0.450, loss: 1.059, lr: 0.11123470522803114\n",
            "epoch: 900, acc: 0.447, loss: 1.059, lr: 0.10010010010010009\n",
            "epoch: 1000, acc: 0.447, loss: 1.059, lr: 0.09099181073703366\n",
            "epoch: 1100, acc: 0.443, loss: 1.059, lr: 0.08340283569641367\n",
            "epoch: 1200, acc: 0.437, loss: 1.059, lr: 0.07698229407236336\n",
            "epoch: 1300, acc: 0.433, loss: 1.059, lr: 0.07147962830593281\n",
            "epoch: 1400, acc: 0.433, loss: 1.059, lr: 0.066711140760507\n",
            "epoch: 1500, acc: 0.427, loss: 1.059, lr: 0.06253908692933083\n",
            "epoch: 1600, acc: 0.427, loss: 1.059, lr: 0.05885815185403177\n",
            "epoch: 1700, acc: 0.423, loss: 1.059, lr: 0.055586436909394105\n",
            "epoch: 1800, acc: 0.423, loss: 1.059, lr: 0.052659294365455495\n",
            "epoch: 1900, acc: 0.423, loss: 1.059, lr: 0.05002501250625312\n",
            "epoch: 2000, acc: 0.423, loss: 1.059, lr: 0.047641734159123386\n",
            "epoch: 2100, acc: 0.430, loss: 1.059, lr: 0.04547521600727603\n",
            "epoch: 2200, acc: 0.433, loss: 1.059, lr: 0.04349717268377555\n",
            "epoch: 2300, acc: 0.430, loss: 1.059, lr: 0.04168403501458941\n",
            "epoch: 2400, acc: 0.430, loss: 1.059, lr: 0.04001600640256102\n",
            "epoch: 2500, acc: 0.430, loss: 1.058, lr: 0.03847633705271258\n",
            "epoch: 2600, acc: 0.430, loss: 1.058, lr: 0.03705075954057058\n",
            "epoch: 2700, acc: 0.430, loss: 1.058, lr: 0.03572704537334762\n",
            "epoch: 2800, acc: 0.430, loss: 1.058, lr: 0.03449465332873405\n",
            "epoch: 2900, acc: 0.430, loss: 1.058, lr: 0.03334444814938312\n",
            "epoch: 3000, acc: 0.427, loss: 1.058, lr: 0.03226847370119393\n",
            "epoch: 3100, acc: 0.427, loss: 1.058, lr: 0.03125976867771178\n",
            "epoch: 3200, acc: 0.427, loss: 1.058, lr: 0.03031221582297666\n",
            "epoch: 3300, acc: 0.423, loss: 1.058, lr: 0.02942041776993233\n",
            "epoch: 3400, acc: 0.423, loss: 1.058, lr: 0.028579594169762787\n",
            "epoch: 3500, acc: 0.423, loss: 1.058, lr: 0.027785495971103084\n",
            "epoch: 3600, acc: 0.423, loss: 1.058, lr: 0.02703433360367667\n",
            "epoch: 3700, acc: 0.423, loss: 1.058, lr: 0.026322716504343247\n",
            "epoch: 3800, acc: 0.420, loss: 1.058, lr: 0.025647601949217745\n",
            "epoch: 3900, acc: 0.420, loss: 1.058, lr: 0.02500625156289072\n",
            "epoch: 4000, acc: 0.417, loss: 1.058, lr: 0.02439619419370578\n",
            "epoch: 4100, acc: 0.417, loss: 1.058, lr: 0.023815194093831864\n",
            "epoch: 4200, acc: 0.417, loss: 1.058, lr: 0.02326122354035822\n",
            "epoch: 4300, acc: 0.417, loss: 1.058, lr: 0.022732439190725165\n",
            "epoch: 4400, acc: 0.420, loss: 1.058, lr: 0.02222716159146477\n",
            "epoch: 4500, acc: 0.420, loss: 1.058, lr: 0.021743857360295715\n",
            "epoch: 4600, acc: 0.420, loss: 1.058, lr: 0.021281123643328365\n",
            "epoch: 4700, acc: 0.420, loss: 1.058, lr: 0.02083767451552407\n",
            "epoch: 4800, acc: 0.420, loss: 1.058, lr: 0.020412329046744233\n",
            "epoch: 4900, acc: 0.420, loss: 1.058, lr: 0.020004000800160033\n",
            "epoch: 5000, acc: 0.420, loss: 1.058, lr: 0.019611688566385566\n",
            "epoch: 5100, acc: 0.420, loss: 1.058, lr: 0.019234468166955183\n",
            "epoch: 5200, acc: 0.420, loss: 1.058, lr: 0.018871485185884128\n",
            "epoch: 5300, acc: 0.420, loss: 1.058, lr: 0.018521948508983144\n",
            "epoch: 5400, acc: 0.423, loss: 1.058, lr: 0.01818512456810329\n",
            "epoch: 5500, acc: 0.423, loss: 1.058, lr: 0.01786033220217896\n",
            "epoch: 5600, acc: 0.423, loss: 1.058, lr: 0.01754693805930865\n",
            "epoch: 5700, acc: 0.423, loss: 1.058, lr: 0.01724435247456458\n",
            "epoch: 5800, acc: 0.423, loss: 1.058, lr: 0.016952025767079167\n",
            "epoch: 5900, acc: 0.423, loss: 1.058, lr: 0.01666944490748458\n",
            "epoch: 6000, acc: 0.423, loss: 1.058, lr: 0.016396130513198885\n",
            "epoch: 6100, acc: 0.423, loss: 1.058, lr: 0.016131634134537828\n",
            "epoch: 6200, acc: 0.427, loss: 1.058, lr: 0.015875535799333228\n",
            "epoch: 6300, acc: 0.427, loss: 1.058, lr: 0.01562744178777934\n",
            "epoch: 6400, acc: 0.427, loss: 1.058, lr: 0.015386982612709646\n",
            "epoch: 6500, acc: 0.427, loss: 1.058, lr: 0.015153811183512654\n",
            "epoch: 6600, acc: 0.427, loss: 1.058, lr: 0.014927601134497688\n",
            "epoch: 6700, acc: 0.427, loss: 1.058, lr: 0.014708045300779527\n",
            "epoch: 6800, acc: 0.427, loss: 1.058, lr: 0.014494854326714018\n",
            "epoch: 6900, acc: 0.427, loss: 1.058, lr: 0.014287755393627663\n",
            "epoch: 7000, acc: 0.427, loss: 1.058, lr: 0.014086491055078181\n",
            "epoch: 7100, acc: 0.423, loss: 1.058, lr: 0.013890818169190166\n",
            "epoch: 7200, acc: 0.423, loss: 1.058, lr: 0.013700506918755994\n",
            "epoch: 7300, acc: 0.423, loss: 1.058, lr: 0.013515339910798757\n",
            "epoch: 7400, acc: 0.423, loss: 1.058, lr: 0.013335111348179758\n",
            "epoch: 7500, acc: 0.423, loss: 1.058, lr: 0.013159626266614028\n",
            "epoch: 7600, acc: 0.423, loss: 1.058, lr: 0.012988699831146902\n",
            "epoch: 7700, acc: 0.423, loss: 1.058, lr: 0.012822156686754713\n",
            "epoch: 7800, acc: 0.423, loss: 1.058, lr: 0.0126598303582732\n",
            "epoch: 7900, acc: 0.423, loss: 1.058, lr: 0.012501562695336917\n",
            "epoch: 8000, acc: 0.423, loss: 1.058, lr: 0.012347203358439314\n",
            "epoch: 8100, acc: 0.423, loss: 1.058, lr: 0.012196609342602758\n",
            "epoch: 8200, acc: 0.423, loss: 1.058, lr: 0.012049644535486204\n",
            "epoch: 8300, acc: 0.423, loss: 1.058, lr: 0.011906179307060364\n",
            "epoch: 8400, acc: 0.423, loss: 1.058, lr: 0.011766090128250382\n",
            "epoch: 8500, acc: 0.423, loss: 1.058, lr: 0.01162925921618793\n",
            "epoch: 8600, acc: 0.423, loss: 1.058, lr: 0.011495574203931488\n",
            "epoch: 8700, acc: 0.423, loss: 1.058, lr: 0.011364927832708264\n",
            "epoch: 8800, acc: 0.423, loss: 1.058, lr: 0.011237217664906169\n",
            "epoch: 8900, acc: 0.423, loss: 1.058, lr: 0.011112345816201801\n",
            "epoch: 9000, acc: 0.423, loss: 1.058, lr: 0.010990218705352238\n",
            "epoch: 9100, acc: 0.423, loss: 1.058, lr: 0.010870746820306556\n",
            "epoch: 9200, acc: 0.423, loss: 1.058, lr: 0.010753844499408539\n",
            "epoch: 9300, acc: 0.423, loss: 1.058, lr: 0.010639429726566656\n",
            "epoch: 9400, acc: 0.423, loss: 1.058, lr: 0.010527423939362039\n",
            "epoch: 9500, acc: 0.423, loss: 1.058, lr: 0.010417751849150954\n",
            "epoch: 9600, acc: 0.423, loss: 1.058, lr: 0.010310341272296112\n",
            "epoch: 9700, acc: 0.423, loss: 1.058, lr: 0.010205122971731808\n",
            "epoch: 9800, acc: 0.423, loss: 1.058, lr: 0.010102030508132133\n",
            "epoch: 9900, acc: 0.423, loss: 1.058, lr: 0.01000100010001\n",
            "epoch: 10000, acc: 0.423, loss: 1.058, lr: 0.009901970492127933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH1vasO-nEb5"
      },
      "source": [
        "## Scenario 4 - SGD (LR 1, decay=1e-3)\n",
        "\n",
        "\n",
        "Better results than scenario 3.\n",
        "\n",
        "But it still should be possible to find parameters that will give us even better results. For example, you may suspect that the initial learning rate is too high. It can make for a great exercise to attempt to find better settings. Feel free to try!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXu8I2_rnEb5"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f9VLQZEnEb5"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFVNySFZnEb5"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0DLPlPKnEb5"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output \n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD(decay=1e-3)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U94AvVaOnEb5"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Remember, each full pass through all of the training data is called an **epoch**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89i63FdqnEb6",
        "outputId": "84fcc35c-0dbf-4b6e-dc81-153e46cabc36"
      },
      "source": [
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "\n",
        "    # Perform a forward pass of our training data through this layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Perform a forward pass through activation function\n",
        "    # takes the output of first dense layer here\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Perform a forward pass through second Dense layer\n",
        "    # takes outputs of activation function of first layer as inputs\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Perform a forward pass through the activation/loss function\n",
        "    # takes the output of second dense layer here and returns loss\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy from output of activation2 and targets\n",
        "    # calculate values along first axis\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions==y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "    \n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.257, loss: 1.099, lr: 1.0\n",
            "epoch: 100, acc: 0.393, loss: 1.085, lr: 0.9099181073703367\n",
            "epoch: 200, acc: 0.383, loss: 1.069, lr: 0.8340283569641367\n",
            "epoch: 300, acc: 0.390, loss: 1.068, lr: 0.7698229407236336\n",
            "epoch: 400, acc: 0.387, loss: 1.067, lr: 0.7147962830593281\n",
            "epoch: 500, acc: 0.393, loss: 1.067, lr: 0.66711140760507\n",
            "epoch: 600, acc: 0.390, loss: 1.066, lr: 0.6253908692933083\n",
            "epoch: 700, acc: 0.390, loss: 1.066, lr: 0.5885815185403178\n",
            "epoch: 800, acc: 0.393, loss: 1.065, lr: 0.5558643690939411\n",
            "epoch: 900, acc: 0.390, loss: 1.063, lr: 0.526592943654555\n",
            "epoch: 1000, acc: 0.397, loss: 1.061, lr: 0.5002501250625312\n",
            "epoch: 1100, acc: 0.400, loss: 1.058, lr: 0.4764173415912339\n",
            "epoch: 1200, acc: 0.413, loss: 1.054, lr: 0.45475216007276037\n",
            "epoch: 1300, acc: 0.423, loss: 1.050, lr: 0.43497172683775553\n",
            "epoch: 1400, acc: 0.427, loss: 1.044, lr: 0.4168403501458941\n",
            "epoch: 1500, acc: 0.440, loss: 1.039, lr: 0.4001600640256102\n",
            "epoch: 1600, acc: 0.437, loss: 1.032, lr: 0.3847633705271258\n",
            "epoch: 1700, acc: 0.437, loss: 1.026, lr: 0.3705075954057058\n",
            "epoch: 1800, acc: 0.450, loss: 1.018, lr: 0.35727045373347627\n",
            "epoch: 1900, acc: 0.467, loss: 1.011, lr: 0.3449465332873405\n",
            "epoch: 2000, acc: 0.473, loss: 1.003, lr: 0.33344448149383127\n",
            "epoch: 2100, acc: 0.460, loss: 0.996, lr: 0.32268473701193934\n",
            "epoch: 2200, acc: 0.450, loss: 0.988, lr: 0.31259768677711786\n",
            "epoch: 2300, acc: 0.453, loss: 0.980, lr: 0.3031221582297666\n",
            "epoch: 2400, acc: 0.473, loss: 0.973, lr: 0.29420417769932333\n",
            "epoch: 2500, acc: 0.463, loss: 0.981, lr: 0.2857959416976279\n",
            "epoch: 2600, acc: 0.460, loss: 0.977, lr: 0.2778549597110308\n",
            "epoch: 2700, acc: 0.473, loss: 0.974, lr: 0.2703433360367667\n",
            "epoch: 2800, acc: 0.473, loss: 0.971, lr: 0.26322716504343247\n",
            "epoch: 2900, acc: 0.490, loss: 0.966, lr: 0.25647601949217746\n",
            "epoch: 3000, acc: 0.487, loss: 0.961, lr: 0.25006251562890724\n",
            "epoch: 3100, acc: 0.487, loss: 0.957, lr: 0.2439619419370578\n",
            "epoch: 3200, acc: 0.483, loss: 0.953, lr: 0.23815194093831865\n",
            "epoch: 3300, acc: 0.483, loss: 0.949, lr: 0.23261223540358225\n",
            "epoch: 3400, acc: 0.490, loss: 0.945, lr: 0.22732439190725165\n",
            "epoch: 3500, acc: 0.490, loss: 0.940, lr: 0.22227161591464767\n",
            "epoch: 3600, acc: 0.497, loss: 0.936, lr: 0.21743857360295715\n",
            "epoch: 3700, acc: 0.493, loss: 0.932, lr: 0.21281123643328367\n",
            "epoch: 3800, acc: 0.490, loss: 0.929, lr: 0.20837674515524068\n",
            "epoch: 3900, acc: 0.497, loss: 0.925, lr: 0.20412329046744235\n",
            "epoch: 4000, acc: 0.497, loss: 0.921, lr: 0.2000400080016003\n",
            "epoch: 4100, acc: 0.497, loss: 0.917, lr: 0.19611688566385566\n",
            "epoch: 4200, acc: 0.497, loss: 0.913, lr: 0.19234468166955185\n",
            "epoch: 4300, acc: 0.500, loss: 0.910, lr: 0.18871485185884126\n",
            "epoch: 4400, acc: 0.500, loss: 0.906, lr: 0.18521948508983144\n",
            "epoch: 4500, acc: 0.500, loss: 0.904, lr: 0.18185124568103292\n",
            "epoch: 4600, acc: 0.503, loss: 0.899, lr: 0.1786033220217896\n",
            "epoch: 4700, acc: 0.503, loss: 0.895, lr: 0.1754693805930865\n",
            "epoch: 4800, acc: 0.513, loss: 0.892, lr: 0.17244352474564578\n",
            "epoch: 4900, acc: 0.527, loss: 0.888, lr: 0.16952025767079165\n",
            "epoch: 5000, acc: 0.530, loss: 0.884, lr: 0.16669444907484582\n",
            "epoch: 5100, acc: 0.527, loss: 0.880, lr: 0.16396130513198884\n",
            "epoch: 5200, acc: 0.533, loss: 0.877, lr: 0.16131634134537828\n",
            "epoch: 5300, acc: 0.537, loss: 0.872, lr: 0.15875535799333226\n",
            "epoch: 5400, acc: 0.540, loss: 0.868, lr: 0.1562744178777934\n",
            "epoch: 5500, acc: 0.560, loss: 0.865, lr: 0.15386982612709646\n",
            "epoch: 5600, acc: 0.563, loss: 0.861, lr: 0.15153811183512653\n",
            "epoch: 5700, acc: 0.563, loss: 0.858, lr: 0.14927601134497687\n",
            "epoch: 5800, acc: 0.577, loss: 0.854, lr: 0.14708045300779526\n",
            "epoch: 5900, acc: 0.593, loss: 0.850, lr: 0.14494854326714016\n",
            "epoch: 6000, acc: 0.597, loss: 0.846, lr: 0.1428775539362766\n",
            "epoch: 6100, acc: 0.603, loss: 0.842, lr: 0.1408649105507818\n",
            "epoch: 6200, acc: 0.613, loss: 0.838, lr: 0.13890818169190167\n",
            "epoch: 6300, acc: 0.617, loss: 0.834, lr: 0.13700506918755992\n",
            "epoch: 6400, acc: 0.617, loss: 0.831, lr: 0.13515339910798757\n",
            "epoch: 6500, acc: 0.620, loss: 0.828, lr: 0.13335111348179757\n",
            "epoch: 6600, acc: 0.617, loss: 0.824, lr: 0.13159626266614027\n",
            "epoch: 6700, acc: 0.623, loss: 0.820, lr: 0.12988699831146902\n",
            "epoch: 6800, acc: 0.627, loss: 0.816, lr: 0.12822156686754713\n",
            "epoch: 6900, acc: 0.620, loss: 0.813, lr: 0.126598303582732\n",
            "epoch: 7000, acc: 0.633, loss: 0.808, lr: 0.12501562695336915\n",
            "epoch: 7100, acc: 0.633, loss: 0.804, lr: 0.12347203358439313\n",
            "epoch: 7200, acc: 0.637, loss: 0.801, lr: 0.12196609342602757\n",
            "epoch: 7300, acc: 0.637, loss: 0.797, lr: 0.12049644535486204\n",
            "epoch: 7400, acc: 0.640, loss: 0.793, lr: 0.11906179307060363\n",
            "epoch: 7500, acc: 0.653, loss: 0.790, lr: 0.11766090128250381\n",
            "epoch: 7600, acc: 0.653, loss: 0.786, lr: 0.11629259216187929\n",
            "epoch: 7700, acc: 0.653, loss: 0.783, lr: 0.11495574203931487\n",
            "epoch: 7800, acc: 0.657, loss: 0.779, lr: 0.11364927832708263\n",
            "epoch: 7900, acc: 0.657, loss: 0.776, lr: 0.11237217664906168\n",
            "epoch: 8000, acc: 0.660, loss: 0.772, lr: 0.11112345816201799\n",
            "epoch: 8100, acc: 0.660, loss: 0.769, lr: 0.10990218705352237\n",
            "epoch: 8200, acc: 0.660, loss: 0.766, lr: 0.10870746820306555\n",
            "epoch: 8300, acc: 0.667, loss: 0.762, lr: 0.1075384449940854\n",
            "epoch: 8400, acc: 0.670, loss: 0.759, lr: 0.10639429726566654\n",
            "epoch: 8500, acc: 0.670, loss: 0.756, lr: 0.10527423939362038\n",
            "epoch: 8600, acc: 0.677, loss: 0.753, lr: 0.10417751849150952\n",
            "epoch: 8700, acc: 0.677, loss: 0.749, lr: 0.10310341272296113\n",
            "epoch: 8800, acc: 0.680, loss: 0.746, lr: 0.1020512297173181\n",
            "epoch: 8900, acc: 0.683, loss: 0.743, lr: 0.10102030508132134\n",
            "epoch: 9000, acc: 0.690, loss: 0.740, lr: 0.1000100010001\n",
            "epoch: 9100, acc: 0.697, loss: 0.736, lr: 0.09901970492127933\n",
            "epoch: 9200, acc: 0.697, loss: 0.732, lr: 0.09804882831650162\n",
            "epoch: 9300, acc: 0.703, loss: 0.729, lr: 0.09709680551509856\n",
            "epoch: 9400, acc: 0.703, loss: 0.726, lr: 0.09616309260505818\n",
            "epoch: 9500, acc: 0.710, loss: 0.722, lr: 0.09524716639679968\n",
            "epoch: 9600, acc: 0.717, loss: 0.719, lr: 0.09434852344560807\n",
            "epoch: 9700, acc: 0.710, loss: 0.716, lr: 0.09346667912889055\n",
            "epoch: 9800, acc: 0.720, loss: 0.713, lr: 0.09260116677470137\n",
            "epoch: 9900, acc: 0.720, loss: 0.710, lr: 0.09175153683824203\n",
            "epoch: 10000, acc: 0.723, loss: 0.707, lr: 0.09091735612328393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5q0xQTUsTfv"
      },
      "source": [
        "## Scenario 5 - SGD (LR 1, decay=1e-3, momentum=0.9)\n",
        "\n",
        "Great result!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss3k9xbpsTfw"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3TQFUNEsTfw"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYMWJeCbsTfw"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2XPUK8VsTfw"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output \n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otR1LqD7sTfw"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Remember, each full pass through all of the training data is called an **epoch**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7jNokmxsvvN",
        "outputId": "1926ee61-2d6e-4a2e-d32c-2da4c49c0e91"
      },
      "source": [
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "\n",
        "    # Perform a forward pass of our training data through this layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Perform a forward pass through activation function\n",
        "    # takes the output of first dense layer here\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Perform a forward pass through second Dense layer\n",
        "    # takes outputs of activation function of first layer as inputs\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Perform a forward pass through the activation/loss function\n",
        "    # takes the output of second dense layer here and returns loss\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy from output of activation2 and targets\n",
        "    # calculate values along first axis\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    accuracy = np.mean(predictions==y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.350, loss: 1.099, lr: 1.0\n",
            "epoch: 100, acc: 0.487, loss: 1.026, lr: 0.9099181073703367\n",
            "epoch: 200, acc: 0.513, loss: 0.931, lr: 0.8340283569641367\n",
            "epoch: 300, acc: 0.597, loss: 0.802, lr: 0.7698229407236336\n",
            "epoch: 400, acc: 0.687, loss: 0.704, lr: 0.7147962830593281\n",
            "epoch: 500, acc: 0.767, loss: 0.563, lr: 0.66711140760507\n",
            "epoch: 600, acc: 0.847, loss: 0.461, lr: 0.6253908692933083\n",
            "epoch: 700, acc: 0.867, loss: 0.411, lr: 0.5885815185403178\n",
            "epoch: 800, acc: 0.637, loss: 0.730, lr: 0.5558643690939411\n",
            "epoch: 900, acc: 0.880, loss: 0.366, lr: 0.526592943654555\n",
            "epoch: 1000, acc: 0.880, loss: 0.352, lr: 0.5002501250625312\n",
            "epoch: 1100, acc: 0.833, loss: 0.440, lr: 0.4764173415912339\n",
            "epoch: 1200, acc: 0.890, loss: 0.325, lr: 0.45475216007276037\n",
            "epoch: 1300, acc: 0.870, loss: 0.323, lr: 0.43497172683775553\n",
            "epoch: 1400, acc: 0.873, loss: 0.325, lr: 0.4168403501458941\n",
            "epoch: 1500, acc: 0.880, loss: 0.307, lr: 0.4001600640256102\n",
            "epoch: 1600, acc: 0.887, loss: 0.299, lr: 0.3847633705271258\n",
            "epoch: 1700, acc: 0.883, loss: 0.296, lr: 0.3705075954057058\n",
            "epoch: 1800, acc: 0.893, loss: 0.292, lr: 0.35727045373347627\n",
            "epoch: 1900, acc: 0.897, loss: 0.289, lr: 0.3449465332873405\n",
            "epoch: 2000, acc: 0.897, loss: 0.287, lr: 0.33344448149383127\n",
            "epoch: 2100, acc: 0.890, loss: 0.285, lr: 0.32268473701193934\n",
            "epoch: 2200, acc: 0.897, loss: 0.283, lr: 0.31259768677711786\n",
            "epoch: 2300, acc: 0.897, loss: 0.281, lr: 0.3031221582297666\n",
            "epoch: 2400, acc: 0.897, loss: 0.279, lr: 0.29420417769932333\n",
            "epoch: 2500, acc: 0.890, loss: 0.277, lr: 0.2857959416976279\n",
            "epoch: 2600, acc: 0.890, loss: 0.276, lr: 0.2778549597110308\n",
            "epoch: 2700, acc: 0.900, loss: 0.275, lr: 0.2703433360367667\n",
            "epoch: 2800, acc: 0.903, loss: 0.273, lr: 0.26322716504343247\n",
            "epoch: 2900, acc: 0.897, loss: 0.272, lr: 0.25647601949217746\n",
            "epoch: 3000, acc: 0.893, loss: 0.270, lr: 0.25006251562890724\n",
            "epoch: 3100, acc: 0.890, loss: 0.268, lr: 0.2439619419370578\n",
            "epoch: 3200, acc: 0.890, loss: 0.266, lr: 0.23815194093831865\n",
            "epoch: 3300, acc: 0.890, loss: 0.265, lr: 0.23261223540358225\n",
            "epoch: 3400, acc: 0.890, loss: 0.264, lr: 0.22732439190725165\n",
            "epoch: 3500, acc: 0.893, loss: 0.264, lr: 0.22227161591464767\n",
            "epoch: 3600, acc: 0.893, loss: 0.263, lr: 0.21743857360295715\n",
            "epoch: 3700, acc: 0.887, loss: 0.262, lr: 0.21281123643328367\n",
            "epoch: 3800, acc: 0.883, loss: 0.261, lr: 0.20837674515524068\n",
            "epoch: 3900, acc: 0.887, loss: 0.261, lr: 0.20412329046744235\n",
            "epoch: 4000, acc: 0.887, loss: 0.260, lr: 0.2000400080016003\n",
            "epoch: 4100, acc: 0.887, loss: 0.260, lr: 0.19611688566385566\n",
            "epoch: 4200, acc: 0.887, loss: 0.259, lr: 0.19234468166955185\n",
            "epoch: 4300, acc: 0.887, loss: 0.259, lr: 0.18871485185884126\n",
            "epoch: 4400, acc: 0.887, loss: 0.258, lr: 0.18521948508983144\n",
            "epoch: 4500, acc: 0.890, loss: 0.258, lr: 0.18185124568103292\n",
            "epoch: 4600, acc: 0.890, loss: 0.257, lr: 0.1786033220217896\n",
            "epoch: 4700, acc: 0.890, loss: 0.257, lr: 0.1754693805930865\n",
            "epoch: 4800, acc: 0.893, loss: 0.256, lr: 0.17244352474564578\n",
            "epoch: 4900, acc: 0.893, loss: 0.256, lr: 0.16952025767079165\n",
            "epoch: 5000, acc: 0.893, loss: 0.256, lr: 0.16669444907484582\n",
            "epoch: 5100, acc: 0.893, loss: 0.255, lr: 0.16396130513198884\n",
            "epoch: 5200, acc: 0.893, loss: 0.255, lr: 0.16131634134537828\n",
            "epoch: 5300, acc: 0.893, loss: 0.254, lr: 0.15875535799333226\n",
            "epoch: 5400, acc: 0.893, loss: 0.254, lr: 0.1562744178777934\n",
            "epoch: 5500, acc: 0.893, loss: 0.254, lr: 0.15386982612709646\n",
            "epoch: 5600, acc: 0.893, loss: 0.253, lr: 0.15153811183512653\n",
            "epoch: 5700, acc: 0.893, loss: 0.253, lr: 0.14927601134497687\n",
            "epoch: 5800, acc: 0.893, loss: 0.253, lr: 0.14708045300779526\n",
            "epoch: 5900, acc: 0.893, loss: 0.252, lr: 0.14494854326714016\n",
            "epoch: 6000, acc: 0.893, loss: 0.252, lr: 0.1428775539362766\n",
            "epoch: 6100, acc: 0.893, loss: 0.252, lr: 0.1408649105507818\n",
            "epoch: 6200, acc: 0.893, loss: 0.252, lr: 0.13890818169190167\n",
            "epoch: 6300, acc: 0.893, loss: 0.251, lr: 0.13700506918755992\n",
            "epoch: 6400, acc: 0.893, loss: 0.251, lr: 0.13515339910798757\n",
            "epoch: 6500, acc: 0.893, loss: 0.251, lr: 0.13335111348179757\n",
            "epoch: 6600, acc: 0.893, loss: 0.250, lr: 0.13159626266614027\n",
            "epoch: 6700, acc: 0.893, loss: 0.250, lr: 0.12988699831146902\n",
            "epoch: 6800, acc: 0.893, loss: 0.250, lr: 0.12822156686754713\n",
            "epoch: 6900, acc: 0.897, loss: 0.249, lr: 0.126598303582732\n",
            "epoch: 7000, acc: 0.897, loss: 0.249, lr: 0.12501562695336915\n",
            "epoch: 7100, acc: 0.897, loss: 0.249, lr: 0.12347203358439313\n",
            "epoch: 7200, acc: 0.897, loss: 0.249, lr: 0.12196609342602757\n",
            "epoch: 7300, acc: 0.897, loss: 0.248, lr: 0.12049644535486204\n",
            "epoch: 7400, acc: 0.897, loss: 0.248, lr: 0.11906179307060363\n",
            "epoch: 7500, acc: 0.897, loss: 0.248, lr: 0.11766090128250381\n",
            "epoch: 7600, acc: 0.897, loss: 0.248, lr: 0.11629259216187929\n",
            "epoch: 7700, acc: 0.897, loss: 0.247, lr: 0.11495574203931487\n",
            "epoch: 7800, acc: 0.897, loss: 0.247, lr: 0.11364927832708263\n",
            "epoch: 7900, acc: 0.897, loss: 0.247, lr: 0.11237217664906168\n",
            "epoch: 8000, acc: 0.897, loss: 0.247, lr: 0.11112345816201799\n",
            "epoch: 8100, acc: 0.897, loss: 0.247, lr: 0.10990218705352237\n",
            "epoch: 8200, acc: 0.897, loss: 0.246, lr: 0.10870746820306555\n",
            "epoch: 8300, acc: 0.897, loss: 0.246, lr: 0.1075384449940854\n",
            "epoch: 8400, acc: 0.897, loss: 0.246, lr: 0.10639429726566654\n",
            "epoch: 8500, acc: 0.897, loss: 0.246, lr: 0.10527423939362038\n",
            "epoch: 8600, acc: 0.897, loss: 0.246, lr: 0.10417751849150952\n",
            "epoch: 8700, acc: 0.897, loss: 0.246, lr: 0.10310341272296113\n",
            "epoch: 8800, acc: 0.897, loss: 0.245, lr: 0.1020512297173181\n",
            "epoch: 8900, acc: 0.897, loss: 0.245, lr: 0.10102030508132134\n",
            "epoch: 9000, acc: 0.897, loss: 0.245, lr: 0.1000100010001\n",
            "epoch: 9100, acc: 0.897, loss: 0.245, lr: 0.09901970492127933\n",
            "epoch: 9200, acc: 0.897, loss: 0.245, lr: 0.09804882831650162\n",
            "epoch: 9300, acc: 0.897, loss: 0.245, lr: 0.09709680551509856\n",
            "epoch: 9400, acc: 0.897, loss: 0.245, lr: 0.09616309260505818\n",
            "epoch: 9500, acc: 0.897, loss: 0.244, lr: 0.09524716639679968\n",
            "epoch: 9600, acc: 0.897, loss: 0.244, lr: 0.09434852344560807\n",
            "epoch: 9700, acc: 0.897, loss: 0.244, lr: 0.09346667912889055\n",
            "epoch: 9800, acc: 0.900, loss: 0.244, lr: 0.09260116677470137\n",
            "epoch: 9900, acc: 0.900, loss: 0.244, lr: 0.09175153683824203\n",
            "epoch: 10000, acc: 0.900, loss: 0.244, lr: 0.09091735612328393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykQftx7Qs0NQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPWsB8d9Cu9x"
      },
      "source": [
        "## Scenario ?? - Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuFgPMTfC1BF"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs61Q9YZC26_"
      },
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtfHxbOyC5Ly"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM3s5RoZC-N_"
      },
      "source": [
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAkDY2SLDG-1"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Remember, each full pass through all of the training data is called an **epoch**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdERi24sDMBq",
        "outputId": "6b1cc497-cd42-4aee-e349-52076c1b4ba3"
      },
      "source": [
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "\n",
        "    # -- ------------------------------\n",
        "    # Forward pass\n",
        "    dense1.forward(X)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # -- ------------------------------\n",
        "    # Get some metrics    \n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions==y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # -- ------------------------------    \n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # -- ------------------------------\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.323, loss: 1.099, lr: 0.05\n",
            "epoch: 100, acc: 0.590, loss: 0.861, lr: 0.04999752512250644\n",
            "epoch: 200, acc: 0.693, loss: 0.725, lr: 0.04999502549496326\n",
            "epoch: 300, acc: 0.763, loss: 0.659, lr: 0.049992526117345455\n",
            "epoch: 400, acc: 0.790, loss: 0.601, lr: 0.04999002698961558\n",
            "epoch: 500, acc: 0.827, loss: 0.556, lr: 0.049987528111736124\n",
            "epoch: 600, acc: 0.807, loss: 0.532, lr: 0.049985029483669646\n",
            "epoch: 700, acc: 0.827, loss: 0.506, lr: 0.049982531105378675\n",
            "epoch: 800, acc: 0.813, loss: 0.493, lr: 0.04998003297682575\n",
            "epoch: 900, acc: 0.830, loss: 0.470, lr: 0.049977535097973466\n",
            "epoch: 1000, acc: 0.847, loss: 0.444, lr: 0.049975037468784345\n",
            "epoch: 1100, acc: 0.820, loss: 0.447, lr: 0.049972540089220974\n",
            "epoch: 1200, acc: 0.837, loss: 0.411, lr: 0.04997004295924593\n",
            "epoch: 1300, acc: 0.843, loss: 0.408, lr: 0.04996754607882181\n",
            "epoch: 1400, acc: 0.813, loss: 0.416, lr: 0.049965049447911185\n",
            "epoch: 1500, acc: 0.830, loss: 0.410, lr: 0.04996255306647668\n",
            "epoch: 1600, acc: 0.857, loss: 0.372, lr: 0.049960056934480884\n",
            "epoch: 1700, acc: 0.857, loss: 0.368, lr: 0.04995756105188642\n",
            "epoch: 1800, acc: 0.833, loss: 0.379, lr: 0.049955065418655915\n",
            "epoch: 1900, acc: 0.863, loss: 0.356, lr: 0.04995257003475201\n",
            "epoch: 2000, acc: 0.837, loss: 0.368, lr: 0.04995007490013731\n",
            "epoch: 2100, acc: 0.857, loss: 0.341, lr: 0.0499475800147745\n",
            "epoch: 2200, acc: 0.863, loss: 0.342, lr: 0.0499450853786262\n",
            "epoch: 2300, acc: 0.853, loss: 0.341, lr: 0.0499425909916551\n",
            "epoch: 2400, acc: 0.867, loss: 0.333, lr: 0.04994009685382384\n",
            "epoch: 2500, acc: 0.867, loss: 0.324, lr: 0.04993760296509512\n",
            "epoch: 2600, acc: 0.863, loss: 0.321, lr: 0.049935109325431604\n",
            "epoch: 2700, acc: 0.850, loss: 0.332, lr: 0.049932615934796004\n",
            "epoch: 2800, acc: 0.887, loss: 0.312, lr: 0.04993012279315098\n",
            "epoch: 2900, acc: 0.883, loss: 0.309, lr: 0.049927629900459285\n",
            "epoch: 3000, acc: 0.880, loss: 0.311, lr: 0.049925137256683606\n",
            "epoch: 3100, acc: 0.867, loss: 0.309, lr: 0.04992264486178666\n",
            "epoch: 3200, acc: 0.877, loss: 0.304, lr: 0.04992015271573119\n",
            "epoch: 3300, acc: 0.877, loss: 0.303, lr: 0.04991766081847992\n",
            "epoch: 3400, acc: 0.863, loss: 0.315, lr: 0.049915169169995596\n",
            "epoch: 3500, acc: 0.887, loss: 0.295, lr: 0.049912677770240964\n",
            "epoch: 3600, acc: 0.873, loss: 0.297, lr: 0.049910186619178794\n",
            "epoch: 3700, acc: 0.873, loss: 0.294, lr: 0.04990769571677183\n",
            "epoch: 3800, acc: 0.853, loss: 0.313, lr: 0.04990520506298287\n",
            "epoch: 3900, acc: 0.887, loss: 0.285, lr: 0.04990271465777467\n",
            "epoch: 4000, acc: 0.890, loss: 0.283, lr: 0.049900224501110035\n",
            "epoch: 4100, acc: 0.863, loss: 0.300, lr: 0.04989773459295174\n",
            "epoch: 4200, acc: 0.880, loss: 0.289, lr: 0.04989524493326262\n",
            "epoch: 4300, acc: 0.873, loss: 0.287, lr: 0.04989275552200545\n",
            "epoch: 4400, acc: 0.857, loss: 0.302, lr: 0.04989026635914307\n",
            "epoch: 4500, acc: 0.863, loss: 0.296, lr: 0.04988777744463829\n",
            "epoch: 4600, acc: 0.867, loss: 0.283, lr: 0.049885288778453954\n",
            "epoch: 4700, acc: 0.857, loss: 0.300, lr: 0.049882800360552884\n",
            "epoch: 4800, acc: 0.887, loss: 0.274, lr: 0.04988031219089794\n",
            "epoch: 4900, acc: 0.857, loss: 0.300, lr: 0.049877824269451976\n",
            "epoch: 5000, acc: 0.850, loss: 0.334, lr: 0.04987533659617785\n",
            "epoch: 5100, acc: 0.887, loss: 0.268, lr: 0.04987284917103844\n",
            "epoch: 5200, acc: 0.897, loss: 0.267, lr: 0.04987036199399661\n",
            "epoch: 5300, acc: 0.893, loss: 0.266, lr: 0.04986787506501525\n",
            "epoch: 5400, acc: 0.897, loss: 0.265, lr: 0.04986538838405724\n",
            "epoch: 5500, acc: 0.893, loss: 0.264, lr: 0.049862901951085496\n",
            "epoch: 5600, acc: 0.897, loss: 0.263, lr: 0.049860415766062906\n",
            "epoch: 5700, acc: 0.887, loss: 0.263, lr: 0.0498579298289524\n",
            "epoch: 5800, acc: 0.877, loss: 0.280, lr: 0.04985544413971689\n",
            "epoch: 5900, acc: 0.897, loss: 0.261, lr: 0.049852958698319315\n",
            "epoch: 6000, acc: 0.877, loss: 0.281, lr: 0.04985047350472258\n",
            "epoch: 6100, acc: 0.890, loss: 0.263, lr: 0.04984798855888967\n",
            "epoch: 6200, acc: 0.897, loss: 0.260, lr: 0.049845503860783506\n",
            "epoch: 6300, acc: 0.867, loss: 0.281, lr: 0.049843019410367055\n",
            "epoch: 6400, acc: 0.897, loss: 0.256, lr: 0.04984053520760327\n",
            "epoch: 6500, acc: 0.877, loss: 0.284, lr: 0.049838051252455155\n",
            "epoch: 6600, acc: 0.903, loss: 0.256, lr: 0.049835567544885655\n",
            "epoch: 6700, acc: 0.897, loss: 0.255, lr: 0.04983308408485778\n",
            "epoch: 6800, acc: 0.877, loss: 0.280, lr: 0.0498306008723345\n",
            "epoch: 6900, acc: 0.897, loss: 0.255, lr: 0.04982811790727884\n",
            "epoch: 7000, acc: 0.883, loss: 0.254, lr: 0.04982563518965381\n",
            "epoch: 7100, acc: 0.897, loss: 0.255, lr: 0.049823152719422406\n",
            "epoch: 7200, acc: 0.900, loss: 0.253, lr: 0.049820670496547675\n",
            "epoch: 7300, acc: 0.897, loss: 0.249, lr: 0.04981818852099264\n",
            "epoch: 7400, acc: 0.897, loss: 0.250, lr: 0.049815706792720335\n",
            "epoch: 7500, acc: 0.857, loss: 0.275, lr: 0.0498132253116938\n",
            "epoch: 7600, acc: 0.903, loss: 0.247, lr: 0.04981074407787611\n",
            "epoch: 7700, acc: 0.867, loss: 0.284, lr: 0.049808263091230306\n",
            "epoch: 7800, acc: 0.887, loss: 0.252, lr: 0.04980578235171948\n",
            "epoch: 7900, acc: 0.903, loss: 0.247, lr: 0.04980330185930667\n",
            "epoch: 8000, acc: 0.877, loss: 0.269, lr: 0.04980082161395499\n",
            "epoch: 8100, acc: 0.903, loss: 0.243, lr: 0.04979834161562752\n",
            "epoch: 8200, acc: 0.883, loss: 0.258, lr: 0.04979586186428736\n",
            "epoch: 8300, acc: 0.887, loss: 0.267, lr: 0.04979338235989761\n",
            "epoch: 8400, acc: 0.900, loss: 0.243, lr: 0.04979090310242139\n",
            "epoch: 8500, acc: 0.900, loss: 0.242, lr: 0.049788424091821805\n",
            "epoch: 8600, acc: 0.897, loss: 0.241, lr: 0.049785945328062006\n",
            "epoch: 8700, acc: 0.900, loss: 0.240, lr: 0.0497834668111051\n",
            "epoch: 8800, acc: 0.900, loss: 0.240, lr: 0.049780988540914256\n",
            "epoch: 8900, acc: 0.907, loss: 0.240, lr: 0.0497785105174526\n",
            "epoch: 9000, acc: 0.900, loss: 0.238, lr: 0.04977603274068329\n",
            "epoch: 9100, acc: 0.880, loss: 0.244, lr: 0.04977355521056952\n",
            "epoch: 9200, acc: 0.897, loss: 0.238, lr: 0.049771077927074414\n",
            "epoch: 9300, acc: 0.890, loss: 0.258, lr: 0.0497686008901612\n",
            "epoch: 9400, acc: 0.897, loss: 0.242, lr: 0.04976612409979302\n",
            "epoch: 9500, acc: 0.887, loss: 0.249, lr: 0.0497636475559331\n",
            "epoch: 9600, acc: 0.897, loss: 0.236, lr: 0.049761171258544616\n",
            "epoch: 9700, acc: 0.880, loss: 0.258, lr: 0.0497586952075908\n",
            "epoch: 9800, acc: 0.893, loss: 0.235, lr: 0.04975621940303483\n",
            "epoch: 9900, acc: 0.900, loss: 0.237, lr: 0.049753743844839965\n",
            "epoch: 10000, acc: 0.877, loss: 0.241, lr: 0.04975126853296942\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}