{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_sgd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTUpl-Ro_QwF"
      },
      "source": [
        "# B\"H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKFxNFJxD6UH"
      },
      "source": [
        "## Note\n",
        "\n",
        "While going thru this notebook, open **`0_full_code_with_various_runs.ipynb`** as well, as we'll refer to there often; see sections:\n",
        "- **SGD optimizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OpJdCXP_SOT"
      },
      "source": [
        "## Optimizers Intro\n",
        "\n",
        "As you will soon discover, most optimizers are just variants of **Stochastic Gradient Descent (SGD)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "753bBgOA_rH1"
      },
      "source": [
        "## SGD Naming Confusion\n",
        "\n",
        "\n",
        "You might hear the following names:\n",
        "- Stochastic Gradient Descent, SGD\n",
        "- Vanilla Gradient Descent, Gradient Descent, GD, or Batch Gradient Descent, BGD\n",
        "- Mini-batch Gradient Descent, MBGD\n",
        "\n",
        "There's a long history of confusion regarding these names.\n",
        "\n",
        "<br>\n",
        " \n",
        "That said, current naming trends and conventions with Stochastic Gradient Descent in use with deep learning today have merged and normalized all of these variants, to the point where we think of the Stochastic Gradient Descent optimizer as one that: **assumes a batch of data**, whether that batch happens to be a **single sample**, **every sample** in a dataset, or **some subset** of the full dataset at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyYvieMWEbrq"
      },
      "source": [
        "## SGD Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PaQJW5C_ixd"
      },
      "source": [
        "### Learning rate\n",
        "\n",
        "We choose a **learning rate**, such as 1.0. \n",
        "- We then **subtract** the `learning_rate * parameter_gradients` from the actual parameter values. \n",
        "- If our learning rate is 1, then weâ€™re subtracting the exact amount of gradient from our parameters. \n",
        "\n",
        "---\n",
        "See code in other notebook. Note how the `update_params` method is given a **layer** object. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvJ0xwti_dAY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}