{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part_04_02_softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhNIGwt4rVxg"
      },
      "source": [
        "### Softmax Activation Function Intro\n",
        "\n",
        "\n",
        "In our case, we’re looking to get this model to be a **classifier**, so we want an activation function (in the final layer) for classification. \n",
        "\n",
        "The softmax activation function can take in non-normalized inputs and produce a normalized distribution of probabilities for our classes. \n",
        "\n",
        "This distribution returned by the softmax activation function represents **confidence scores** for each class and will add up to 1. \n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://drive.google.com/uc?id=1OngR478lvQM7OWBvvKOo_5lX_J8TKf8U)\n",
        "\n",
        "<br>\n",
        "\n",
        "- The numerator exponentiates the current output value \n",
        "- The denominator takes a sum of all of the exponentiated outputs for a given sample\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUDj4dgOsvDT"
      },
      "source": [
        "### Softmax Function Code\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7SkgMOnTwvh_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDyeouVSswG7"
      },
      "source": [
        "def softmax(logits):\n",
        "\n",
        "    denominator = 0\n",
        "    for logit in logits:\n",
        "        denominator += math.pow(math.e, logit)\n",
        "\n",
        "    probs = []\n",
        "    for logit in logits:\n",
        "        probs.append(\n",
        "            math.pow(math.e, logit) / denominator\n",
        "        )\n",
        "\n",
        "    return probs\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logits** here simply refers to the unscaled output of the earlier network layers. The sum of the inputs does not have to equal 1 and the values are not probabilities.\n",
        "\n",
        "As for why they're called logits, we'll leave that for another time ..."
      ],
      "metadata": {
        "id": "QxKMutaywwxK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-KaSGkmpvmL"
      },
      "source": [
        "### Cat/dog example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJKv09S0tSW9",
        "outputId": "10339ec0-f61b-436a-92a2-5e2e6e6c9b59"
      },
      "source": [
        "logits = [.9, .7]\n",
        "\n",
        "CAT_IDX = 0\n",
        "DOG_IDX = 1\n",
        "\n",
        "probs = softmax(logits)\n",
        "\n",
        "print(f'''\n",
        "    Cat: {probs[CAT_IDX]} \n",
        "    Dog: {probs[DOG_IDX]}\n",
        "    SUM: {np.array(probs).sum()}\n",
        "''')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Cat: 0.549833997312478 \n",
            "    Dog: 0.4501660026875221\n",
            "    SUM: 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w9Xp_h7pybd"
      },
      "source": [
        "### Example with 4 logits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAIreUMPsGYb",
        "outputId": "0f37a1ca-72e1-483e-a2e2-8d6e6f375101"
      },
      "source": [
        "logits = [.9, .7, .6, .0001]\n",
        "\n",
        "HOUSE_IDX = 0\n",
        "TREE_IDX = 1\n",
        "LIZARD_IDX = 2\n",
        "SKUNK_IDX = 3\n",
        "\n",
        "probs = softmax(logits)\n",
        "\n",
        "print(f'''\n",
        "    House : {probs[HOUSE_IDX]} \n",
        "    Tree  : {probs[TREE_IDX]}\n",
        "    Lizard: {probs[LIZARD_IDX]}\n",
        "    Skunk : {probs[SKUNK_IDX]}\n",
        "    SUM   : {np.array(probs).sum()}\n",
        "''')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    House : 0.3371363104229755 \n",
            "    Tree  : 0.276023865322535\n",
            "    Lizard: 0.24975672161474802\n",
            "    Skunk : 0.13708310263974147\n",
            "    SUM   : 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3vz86yBsGcs"
      },
      "source": [
        "### Why use softmax as opposed to standard normalization?\n",
        "\n",
        "'Tis a good question, but not to be answered here :)\n",
        "\n",
        "In short, Softmax works better with neural networks, especially when looking how this function feeds into the classification loss function (cross-entropy loss) - they play well together.\n",
        "\n",
        "For more details see [here](https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdefqN9BsDt0"
      },
      "source": [
        "### Sigmoid Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax is the generalization of the sigmoid for multi-class problems.\n",
        "\n",
        "If only a single logit is being passed into the final layer's activation function, do not use Softmax - it will just return 1 :)"
      ],
      "metadata": {
        "id": "3slvWGOZ2B5T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apLQPTcGrZOW",
        "outputId": "ef5a18d5-6ba0-4fe9-c7da-363832d553c4"
      },
      "source": [
        "for x in np.arange(-5, 5):\n",
        "    print(softmax([x])[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather, use the Sigmoid function. It returns a value in the range of: \n",
        "- 0 for negative infinity\n",
        "- through 0.5 for the input of 0\n",
        "- to 1 for positive infinity\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=17laVogv5ny9jS_QO8aXDlsI5FBOwGuMt)\n",
        "\n"
      ],
      "metadata": {
        "id": "DJCAD-UL2e5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.pow(math.e, -x))"
      ],
      "metadata": {
        "id": "hJVVENGI3Brq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in np.arange(-5, 5):\n",
        "    print(sigmoid(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDIXQmil3Y3N",
        "outputId": "5e908d74-ffd2-4162-8656-9620ad6b5131"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.006692850924284857\n",
            "0.017986209962091562\n",
            "0.04742587317756679\n",
            "0.11920292202211757\n",
            "0.2689414213699951\n",
            "0.5\n",
            "0.7310585786300049\n",
            "0.8807970779778823\n",
            "0.9525741268224331\n",
            "0.9820137900379085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note on Exponentiation"
      ],
      "metadata": {
        "id": "WnRq2zKOqc-s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLCF8G2ohgU9"
      },
      "source": [
        "\n",
        "\n",
        "Exponentiation serves multiple purposes. \n",
        "- An exponential value of any number is always nonnegative\n",
        "- The exponential function is a **monotonic function**: with higher input values, outputs are also higher, so we won’t change the predicted class after applying it while making sure that we get non-negative values. \n",
        "\n",
        "<br>\n",
        "\n",
        "$\\large e^x$ returns: \n",
        "- 0 for negative infinity \n",
        "- 1 for the input of 0 \n",
        "- increases for positive values\n",
        "\n",
        "![](https://drive.google.com/uc?id=1HroSgcamG-yFwdbvLTa30Ku9xdG8onx8)\n",
        "\n",
        "<br>\n",
        "\n",
        "In addition, in the softmax activation function, the denominator takes a sum of all of the exponentiated outputs: this adds stability to the result as the **normalized exponentiation** is more about the difference between numbers than their magnitudes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjnxc6QCFZhw"
      },
      "source": [
        "### Note on Translational Invariance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a just side note. \n",
        "\n",
        "No need to worry about it too much for now. Just keep in mind that Softmax is translationally invariant.\n",
        "\n",
        "---\n",
        "\n",
        "In short, instead of thinking about the output as actual class probabilities, view it as an indication based on the scores, which class is the most likely. "
      ],
      "metadata": {
        "id": "eKqiFNAMFZhx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54003298-bc87-4dda-e39e-ea8693899c3e",
        "id": "9sQZe750FZhx"
      },
      "source": [
        "print(softmax([1, 4]))\n",
        "print(softmax([101, 104]))\n",
        "print(softmax([-101, -104]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.04742587317756678, 0.9525741268224331]\n",
            "[0.04742587317756678, 0.9525741268224331]\n",
            "[0.9525741268224331, 0.04742587317756678]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBx5itKNFZhx"
      },
      "source": [
        "Depending on the problem, it can make sense to add a **_non/none_** class. \n",
        "\n",
        "For example, imagine: \n",
        "- index 0: none\n",
        "- index 1: cat\n",
        "- index 2: dog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3a02a6c-87cc-46e7-fb90-86705ae0b422",
        "id": "B3xs-_F6FZhx"
      },
      "source": [
        "print(f'{np.round(softmax([0, 1, 4]), 5)}')\n",
        "print(f'{np.round(softmax([0, 101, 104]), 5)}')\n",
        "print(f'{np.round(softmax([0, -101, -104]), 5)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01715 0.04661 0.93624]\n",
            "[0.      0.04743 0.95257]\n",
            "[1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qph-WeKOCaMb"
      },
      "source": [
        "### Exploding Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG-iAODm3es1"
      },
      "source": [
        "\n",
        "Two of the pervasive challenges with neural networks: “dead neurons”\n",
        "and very large numbers (referred to as “exploding” values). \n",
        "\n",
        "“Dead” neurons and enormous numbers can wreak havoc down the line and render a network useless over time. \n",
        "\n",
        "---\n",
        "\n",
        "The exponential function used in softmax activation is one of the sources of exploding values. \n",
        "\n",
        "We know the exponential function tends toward 0 as its input value approaches negative infinity, and the output is 1 when the input is 0. \n",
        "\n",
        "We can use this property to prevent the exponential function from overflowing. \n",
        "\n",
        "Let's subtract the maximum value from the list of input values. That would then change the output values to always be in a range from some negative value up to 0, as the largest number subtracted by itself returns 0, and any smaller number subtracted by it will result in a negative number - exactly the range discussed above. \n",
        "\n",
        "<br>\n",
        "\n",
        "With Softmax, thanks to the normalization, we can subtract any\n",
        "value from all of the inputs, and it will not change the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2GUozgpEgH9",
        "outputId": "2e2e8aae-caea-400a-a9de-a705430b4f8c"
      },
      "source": [
        "print(f'''\n",
        "    exp(-np.inf): {np.exp(-np.inf)}\n",
        "    exp(-3)     : {np.exp(-3)}\n",
        "    exp(-2)     : {np.exp(-2)}\n",
        "    exp(-1)     : {np.exp(-1)}\n",
        "    exp(-.5)    : {np.exp(-.5)}\n",
        "    exp(0)      : {np.exp(0)}\n",
        "'''    \n",
        ")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    exp(-np.inf): 0.0\n",
            "    exp(-3)     : 0.049787068367863944\n",
            "    exp(-2)     : 0.1353352832366127\n",
            "    exp(-1)     : 0.36787944117144233\n",
            "    exp(-.5)    : 0.6065306597126334\n",
            "    exp(0)      : 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0f89fMp78dU"
      },
      "source": [
        "#### Example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "\n",
        "    # two new lines added:\n",
        "    logits = np.array(logits)\n",
        "    logits = logits - logits.max()\n",
        "\n",
        "    denominator = 0\n",
        "    for logit in logits:\n",
        "        denominator += math.pow(math.e, logit)\n",
        "\n",
        "    probs = []\n",
        "    for logit in logits:\n",
        "        probs.append(\n",
        "            math.pow(math.e, logit) / denominator\n",
        "        )\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "5nZoM_vT7_fW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a1d978-f4af-4fb5-c35e-0e4b0b022972",
        "id": "NnpeKyyH78dU"
      },
      "source": [
        "logits = [9999999, 9999990, 9999960, 9999943]\n",
        "\n",
        "HOUSE_IDX = 0\n",
        "TREE_IDX = 1\n",
        "LIZARD_IDX = 2\n",
        "SKUNK_IDX = 3\n",
        "\n",
        "probs = softmax(logits)\n",
        "\n",
        "print(f'''\n",
        "    House : {probs[HOUSE_IDX]} \n",
        "    Tree  : {probs[TREE_IDX]}\n",
        "    Lizard: {probs[LIZARD_IDX]}\n",
        "    Skunk : {probs[SKUNK_IDX]}\n",
        "    SUM   : {np.array(probs).sum()}\n",
        "''')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    House : 0.9998766054240137 \n",
            "    Tree  : 0.00012339457598623178\n",
            "    Lizard: 1.1546799184790585e-17\n",
            "    Skunk : 4.78030294763524e-25\n",
            "    SUM   : 0.9999999999999999\n",
            "\n"
          ]
        }
      ]
    }
  ]
}