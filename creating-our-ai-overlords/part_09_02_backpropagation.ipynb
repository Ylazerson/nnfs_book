{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part_09_02_backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prRXQ0p94hzC"
      },
      "source": [
        "### Intro\n",
        "\n",
        "\n",
        "<u>**Scenario A: 1 neuron in current layer, 1 in next layer**</u>\n",
        "\n",
        "So far, we have performed an example backward pass with a **single neuron**, which received a **singular derivative**, `deriv_from_next_layer` (from \"next\" layer) to apply the chain rule. \n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<u>**Scenario B: 1 neuron in current layer, mulitple in next layer**</u>\n",
        "\n",
        "Let’s consider **multiple neurons** in the next layer. A **single neuron** of the current layer connects to all of them — they all receive the output of this neuron. \n",
        "\n",
        "What will happen during backpropagation? \n",
        "- Each neuron from the next layer will return a partial derivative of its function with respect to this input. \n",
        "- The neuron in the current layer will receive a vector consisting of these derivatives. \n",
        "- We need this to be a **singular value** for a singular neuron. \n",
        "- To continue backpropagation, we need to **sum** this vector.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<u>**Scenario C: Multiple neurons in current layer, mulitple in next layer**</u>\n",
        "\n",
        "During backpropagation: \n",
        "- Each neuron from the current layer will receive a vector of partial derivatives the same way that we described for Scenario B. \n",
        "- With a layer of neurons, it’ll take the form of a list of these vectors, or a 2D array. \n",
        "- Each neuron in the next layer is going to output a gradient of the partial derivatives with respect to all of its inputs. \n",
        "- From all the neurons in the next layer this will form a list of these vectors. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al3mmHX1pz2H"
      },
      "source": [
        "### Our staged scenario\n",
        "\n",
        "We're zooming into a layer of 3 neurons with 4 inputs. Let's image we have done the forward pass already and are in middle of the backpropagation and have the gradients for the \"next layer/sublayer\" already. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpYyDAb-JdL8"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvDuHyHDKj8A"
      },
      "source": [
        "#### Current layer and gradient from next"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgdB5qYUuY9q"
      },
      "source": [
        "# -- ------------------------------\n",
        "# Passed-in gradient from the next layer/sublayer\n",
        "dvalues = np.array([[1., 1., 1.]])\n",
        "\n",
        "# We have 3 sets of weights - one set for each neuron\n",
        "# we have 4 inputs, thus 4 weights\n",
        "# recall that we keep weights transposed\n",
        "weights = np.array([\n",
        "    [0.2, 0.8, -0.5, 1],       # weights for neuron 1\n",
        "    [0.5, -0.91, 0.26, -0.5],  # weights for neuron 2\n",
        "    [-0.26, -0.27, 0.17, 0.87] # weights for neuron 3\n",
        "]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZnig9bnNnYo",
        "outputId": "593698bb-2624-4530-80c9-4903ef05926f"
      },
      "source": [
        "dvalues"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c97l2K7qNovl",
        "outputId": "9b33f420-c1a4-47df-8c7d-91b253627be0"
      },
      "source": [
        "dvalues.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgXLSfVLODwD"
      },
      "source": [
        "<u>**Remember:**</u> `weights` has been transposed:\n",
        "- Before each \"record\" represented a neuron and its weights\n",
        "- Now each \"record\" represents an input attribute, of which there are 4, and its 3 weights (one for each neuron) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cS5wtl3Nqll",
        "outputId": "d58cdf91-edcd-49a1-d032-cca57ecc127c"
      },
      "source": [
        "weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.2 ,  0.5 , -0.26],\n",
              "       [ 0.8 , -0.91, -0.27],\n",
              "       [-0.5 ,  0.26,  0.17],\n",
              "       [ 1.  , -0.5 ,  0.87]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAfLDLL5NvmF",
        "outputId": "77dbbae1-4ed6-496d-b671-6cea6e325f77"
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24n6KFV3uRGW"
      },
      "source": [
        "#### Get gradient\n",
        "\n",
        "Remember from above, in section _**Sublayer - weights and inputs**_, that to calculate the partial derivative **with respect to the input** equals the related weight. \n",
        "\n",
        "> Note: `dinputs` is a gradient of the neuron function with respect to inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwAJjjMDKu9G",
        "outputId": "1c787f1f-1e13-487a-a555-07c65097da5b"
      },
      "source": [
        "# Sum weights related to the given input multiplied by\n",
        "# the gradient related to the given neuron\n",
        "dx0 = sum([\n",
        "    weights[0][0]*dvalues[0][0], \n",
        "    weights[0][1]*dvalues[0][1],\n",
        "    weights[0][2]*dvalues[0][2]\n",
        "])\n",
        "\n",
        "dx1 = sum([\n",
        "    weights[1][0]*dvalues[0][0], \n",
        "    weights[1][1]*dvalues[0][1],\n",
        "    weights[1][2]*dvalues[0][2]\n",
        "])\n",
        "\n",
        "dx2 = sum([\n",
        "    weights[2][0]*dvalues[0][0], \n",
        "    weights[2][1]*dvalues[0][1],\n",
        "    weights[2][2]*dvalues[0][2]\n",
        "])\n",
        "\n",
        "dx3 = sum([\n",
        "    weights[3][0]*dvalues[0][0], \n",
        "    weights[3][1]*dvalues[0][1],\n",
        "    weights[3][2]*dvalues[0][2]\n",
        "])\n",
        "\n",
        "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
        "\n",
        "print(dinputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.44 -0.38 -0.07  1.37]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ziNIOuXPghT"
      },
      "source": [
        "#### Using dot product\n",
        "\n",
        "We can achieve the same result by using the `np.dot`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ56ACyVPxd0",
        "outputId": "23002a77-fc55-4a36-a1a3-f3c6d6c795e5"
      },
      "source": [
        "# sum weights of given input\n",
        "# and multiply by the passed-in gradient for this neuron\n",
        "dinputs = np.dot(dvalues[0], weights.T)\n",
        "\n",
        "print(dinputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.44 -0.38 -0.07  1.37]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA4sk1OlQG84"
      },
      "source": [
        "#### Using batch of inputs\n",
        "\n",
        "With more samples, the \"next\" layer will return a list of gradients. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjXJOL8Sfbh"
      },
      "source": [
        "##### wrt inputs\n",
        "\n",
        "Our code just needs a minor tweak: `dvalues` instead of `dvalues[0]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4-ytPe3QKR8",
        "outputId": "ef8c6412-b7e0-4f42-b42d-2bbf1d6fad4b"
      },
      "source": [
        "# Passed-in gradient from the next layer\n",
        "# for the purpose of this example we're going to use\n",
        "# an array of an incremental gradient values\n",
        "dvalues = np.array([[1., 1., 1.],\n",
        "                    [2., 2., 2.],\n",
        "                    [3., 3., 3.]])\n",
        "\n",
        "# SAME AS BEFORE\n",
        "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
        "                    [0.5, -0.91, 0.26, -0.5],\n",
        "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
        "\n",
        "# sum weights of given input\n",
        "# and multiply by the passed-in gradient for this neuron\n",
        "dinputs = np.dot(dvalues, weights.T)\n",
        "\n",
        "print(dinputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.44 -0.38 -0.07  1.37]\n",
            " [ 0.88 -0.76 -0.14  2.74]\n",
            " [ 1.32 -1.14 -0.21  4.11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6sJIbJDQjXJ"
      },
      "source": [
        "##### wrt weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DueBaGcWSsIO"
      },
      "source": [
        "# We have 3 sets of inputs - samples\n",
        "inputs = np.array([[1, 2, 3, 2.5],\n",
        "                   [2., 5., -1., 2],\n",
        "                   [-1.5, 2.7, 3.3, -0.8]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w95v6dmuT6pu",
        "outputId": "98d43937-abcc-4634-ff7b-975c0be105ee"
      },
      "source": [
        "# Let's remember what dvalues looks like:\n",
        "dvalues"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1.],\n",
              "       [2., 2., 2.],\n",
              "       [3., 3., 3.]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7XXvCkTxeN",
        "outputId": "d541d20c-eea2-414a-ffaa-a4847a87bd54"
      },
      "source": [
        "# inputs transposed (imagine now that \"row\" one has all the 3 eyecolors of the 3 people being inputted)\n",
        "inputs.T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1. ,  2. , -1.5],\n",
              "       [ 2. ,  5. ,  2.7],\n",
              "       [ 3. , -1. ,  3.3],\n",
              "       [ 2.5,  2. , -0.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgYikW57TwcU",
        "outputId": "c4bb74e4-de5b-4c16-f4a1-ff001d46710c"
      },
      "source": [
        "# sum inputs for given weight\n",
        "# and multiply by the passed-in gradient for this neuron\n",
        "dweights = np.dot(inputs.T, dvalues)\n",
        "\n",
        "print(dweights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.5  0.5  0.5]\n",
            " [20.1 20.1 20.1]\n",
            " [10.9 10.9 10.9]\n",
            " [ 4.1  4.1  4.1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v_HqRSFSzse"
      },
      "source": [
        "##### wrt to biases\n",
        "\n",
        "For the biases and derivatives with respect to them, the derivatives come from the sum operation and always equal 1, multiplied by the incoming gradients to apply the chain rule. \n",
        "\n",
        "Since gradients are a list of gradients (a vector of gradients for each neuron for all samples), we just have to sum them with the neurons, column-wise, along axis 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6ZQ8sRcUu0f",
        "outputId": "212c5332-53f4-42d6-b344-3c8b81f14b4b"
      },
      "source": [
        "# One bias for each neuron\n",
        "# biases are the row vector with a shape (1, neurons)\n",
        "biases = np.array([[2, 3, 0.5]])\n",
        "\n",
        "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "print(dbiases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6. 6. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDoRTSE2UbaW"
      },
      "source": [
        "##### Sublayer ReLU\n",
        "\n",
        "Let's now move up to the next sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8EH2-SwVIKw"
      },
      "source": [
        "# Example layer output (before ReLU)\n",
        "z = np.array([[1, 2, -3, -4],\n",
        "              [2, -7, -1, 3],\n",
        "              [-1, 2, 5, -1]])\n",
        "\n",
        "# Gradient from \"next\" layer\n",
        "dvalues = np.array([[1, 2, 3, 4],\n",
        "                    [5, 6, 7, 8],\n",
        "                    [9, 10, 11, 12]])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTAJMDO0avEn",
        "outputId": "7dd3d6cf-ba39-4825-8183-23fab64ef30e"
      },
      "source": [
        "# ReLU activation's derivative\n",
        "drelu = np.zeros_like(z)\n",
        "# Set cells to 1 where corresponding z cell > 0\n",
        "drelu[z > 0] = 1\n",
        "\n",
        "print(drelu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 0 0]\n",
            " [1 0 0 1]\n",
            " [0 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4Xwbn8az9A",
        "outputId": "833cb1bf-24e4-4f4e-9845-2659f83136ef"
      },
      "source": [
        "# The chain rule\n",
        "drelu *= dvalues\n",
        "\n",
        "print(drelu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  2  0  0]\n",
            " [ 5  0  0  8]\n",
            " [ 0 10 11  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esaKECPJbO_Y"
      },
      "source": [
        "###### Simplified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAM1F5HmbhWL",
        "outputId": "c1e72996-8820-4a00-9135-cd2d0db07cc1"
      },
      "source": [
        "drelu = dvalues.copy()\n",
        "drelu[z <= 0] = 0\n",
        "\n",
        "print(drelu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  2  0  0]\n",
            " [ 5  0  0  8]\n",
            " [ 0 10 11  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-BbjZ7Ialdu"
      },
      "source": [
        "#### Full forward and backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALfHZvs2cEdq"
      },
      "source": [
        "# Passed-in gradient from the next layer\n",
        "# for the purpose of this example we're going to use\n",
        "# an array of an incremental gradient values\n",
        "dvalues = np.array([[1., 1., 1.],\n",
        "                    [2., 2., 2.],\n",
        "                    [3., 3., 3.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlSRP4mWcHMK"
      },
      "source": [
        "# We have 3 sets of inputs - samples\n",
        "inputs = np.array([[1, 2, 3, 2.5],\n",
        "                   [2., 5., -1., 2],\n",
        "                   [-1.5, 2.7, 3.3, -0.8]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2LHevTRcJU9"
      },
      "source": [
        "# We have 3 sets of weights - one set for each neuron\n",
        "# we have 4 inputs, thus 4 weights\n",
        "# recall that we keep weights transposed\n",
        "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
        "                    [0.5, -0.91, 0.26, -0.5],\n",
        "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
        "\n",
        "\n",
        "# One bias for each neuron\n",
        "biases = np.array([[2, 3, 0.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPqZ_rE2cSXX"
      },
      "source": [
        "# Forward pass\n",
        "layer_outputs = np.dot(inputs, weights) + biases  # Dense layer\n",
        "relu_outputs = np.maximum(0, layer_outputs)  # ReLU activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JHxrzSqcUcv"
      },
      "source": [
        "# Backpropagation starts here\n",
        "\n",
        "# ReLU activation - simulates derivative with respect to input values\n",
        "# from next layer passed to current layer during backpropagation\n",
        "drelu = relu_outputs.copy()\n",
        "drelu[layer_outputs <= 0] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJw1Ickpchq-"
      },
      "source": [
        "# dinputs - multiply by weights\n",
        "dinputs = np.dot(drelu, weights.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA-Ri-QkclCw"
      },
      "source": [
        "# dweights - multiply by inputs\n",
        "dweights = np.dot(inputs.T, drelu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-XOgvoPcmdf"
      },
      "source": [
        "# dbiases - sum values, do this over samples (first axis)\n",
        "dbiases = np.sum(drelu, axis=0, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CZMw6Cob3js",
        "outputId": "2dea6ce8-5c47-4b98-e1cb-4be481b89623"
      },
      "source": [
        "# Update parameters\n",
        "weights += -0.001 * dweights\n",
        "biases += -0.001 * dbiases\n",
        "\n",
        "print(weights)\n",
        "print(biases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.179515   0.5003665 -0.262746 ]\n",
            " [ 0.742093  -0.9152577 -0.2758402]\n",
            " [-0.510153   0.2529017  0.1629592]\n",
            " [ 0.971328  -0.5021842  0.8636583]]\n",
            "[[1.98489  2.997739 0.497389]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l1l74qialiI"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "By this point, we’ve covered everything we need to perform backpropagation, except for the derivative of the Softmax activation function and the derivative of the cross-entropy loss function.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}