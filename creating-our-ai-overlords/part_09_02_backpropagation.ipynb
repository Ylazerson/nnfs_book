{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part_09_01_backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQhyUTvpWmHQ"
      },
      "source": [
        "## Singel Neuron \"Network\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0CRppjbW1HV"
      },
      "source": [
        "Before applying this to a complete neural network, let’s start with a simplified forward pass with just one neuron. Rather than backpropagating from the loss function for a full neural network, let’s backpropagate the ReLU function for a single neuron and act as if we intend to **minimize the output for this single neuron**. \n",
        "\n",
        "This example is obviously not used in the real world (where we minimize the loss etc) - this just for learning purposes etc.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1_lq5wWBDiqhtXbPaOTfvdCrZY0o7qJQz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPtUyj5XXC4-"
      },
      "source": [
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'xw0 :{xw0}')\n",
        "print(f'xw1 :{xw1}')\n",
        "print(f'xw2 :{xw2}')\n",
        "print(f'z   :{z}')\n",
        "print(f'y   :{y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dp8HzddnT1V",
        "outputId": "ff7daf80-d987-4ed6-e5a8-685507f0f5cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xw0 :-3.0\n",
            "xw1 :2.0\n",
            "xw2 :6.0\n",
            "z   :6.0\n",
            "y   :6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug_oYdClXgWJ"
      },
      "source": [
        "### Our Big Function\n",
        "\n",
        "![](https://drive.google.com/uc?id=1JpU0VqoiiRBLoDyrV7GE1JSED14cwPxo)\n",
        "\n",
        "<br>\n",
        "\n",
        "Let’s rewrite our equation to the form that will allow us to determine how to calculate the derivatives more easily:\n",
        "\n",
        "![](https://drive.google.com/uc?id=17XPz2uAdSPCmTwHn6D0b5I6BDCHZudmP)\n",
        "\n",
        "<br>\n",
        "\n",
        "... in psuedo-code:\n",
        "\n",
        "```\n",
        "ReLU(\n",
        "    sum(\n",
        "        mul(x0, w0), \n",
        "        mul(x1, w1), \n",
        "        mul(x2, w2), \n",
        "        b\n",
        "    )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynyjtkGIZyFj"
      },
      "source": [
        "### Partial derivative of x0\n",
        "\n",
        "Let's start by considering what we need to calculate for the partial derivative of $\\large x_0$\n",
        "\n",
        "![](https://drive.google.com/uc?id=1JdTSOrQXda3c6a6LOoFHZRUB2HnFP4X5)\n",
        "\n",
        "\n",
        "> For legibility, we did not denote the $\\large ReLU()$ parameter (which is the full sum), nor the $\\large sum()$ parameters (which are all of the multiplications of inputs and weights). We excluded this because the equation would be longer and harder to read. \n",
        "\n",
        "This equation shows that we have to calculate the derivatives and partial derivatives of all of the atomic operations and multiply them to acquire the impact that $\\large x_0$ makes on the output. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74_03ZrpbLvS"
      },
      "source": [
        "#### Gradient from next layer\n",
        "\n",
        "We’ll have multiple chained layers of neurons in the neural network model, followed by the loss function. \n",
        "\n",
        "We'll want to know the impact of a given **weight or bias** on the loss. \n",
        "\n",
        "The derivative **with respect to the layer’s inputs**, as opposed to the derivative **with respect to the weights and biases**, is not used to update any parameters. Instead, it is used to **chain** to another layer (which is why we backpropagate to the previous layer in a chain).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For this example, let’s assume that our neuron receives a gradient of $1$ from the **next layer**. We’re making up this value for demonstration purposes, and a value of 1 won’t change the values, which means that we can more easily show all of the processes. \n",
        "\n",
        "We are going to use the color of red for derivatives.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1kdRsvEPwknm-mTohSuPzUrv2S6MCYml-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10YzeN2ndbKQ"
      },
      "source": [
        "#### Sublayer - ReLU\n",
        "\n",
        "Recall that the derivative of ReLU() **with respect to its input** is 1, if the input is greater than 0, and 0 otherwise.\n",
        "\n",
        "The input value to the ReLU function is 6, so the derivative equals 1. \n",
        "\n",
        "We have to use the chain rule and multiply this derivative with the derivative received from the next layer (which we made up to be 1).\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://drive.google.com/uc?id=1ZBCFRJbpnniz3uZO5vEFC0C_xEPs7LXk)\n",
        "\n",
        "This results with the derivative of 1:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1fyBKyZ3Bz-nwGfbgRvbZGFJzojt4fyok)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: `w.r.t` stands for \"with respect to\""
      ],
      "metadata": {
        "id": "GRtcWcl3Cavn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4yGocJadzDt",
        "outputId": "1fbcb444-abc5-49a0-d2fb-bdd37e356dc6"
      },
      "source": [
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "print(deriv_relu_wrt_z)\n",
        "# -- ---------------------------------------\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q61L1oIe9Vl"
      },
      "source": [
        "#### Sublayer - Sum\n",
        "\n",
        "The partial derivative of the simple sum operation (i.e. $f(x, y, z) = x + y + z$) is always 1, no matter the inputs:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1CK16QPZ9QmMF-GQzC44ApNYF2Nm--s7m)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqHyMkFHhnDI",
        "outputId": "68eaa487-c745-4ef4-8538-df9846e316ce"
      },
      "source": [
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the sum with chain rule\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "\n",
        "# -- ----------------------\n",
        "\n",
        "print(f'deriv_relu_wrt_z  : {deriv_relu_wrt_z}')\n",
        "print(f'deriv_relu_wrt_xw0: {deriv_relu_wrt_xw0}')\n",
        "# -- ---------------------------------------\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deriv_relu_wrt_z  : 1.0\n",
            "deriv_relu_wrt_xw0: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwdNYwm5g0nP"
      },
      "source": [
        "\n",
        "![](https://drive.google.com/uc?id=1bLDvheKGVXgleqzgrIBx_tCfC-zoBxkF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2OlmAuZj93O"
      },
      "source": [
        "### For all weighted inputs and bias\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Similarity of the partial derivatives"
      ],
      "metadata": {
        "id": "d1m63T1ZGfqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note how all the partial derivatives of this \"Network\" are similar. Due to the chain rule they end up sharing a lot of the same parts.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1_XloSiT5xFdoJMLBFXjJvC8wNXbXHEK_)\n",
        "\n",
        "> For legibility, we did not denote the $\\large ReLU()$ parameter (which is the full sum), nor the $\\large sum()$ parameters (which are all of the multiplications of inputs and weights). We excluded this because the equation would be longer and harder to read. "
      ],
      "metadata": {
        "id": "5UxHjm9zE9Hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the complete one for $\\large x_0$:\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\huge \\frac{∂}{∂ x_0} = $ \n",
        "\n",
        "```python\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "# Partial derivatives of the sum, the chain rule\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "\n",
        "# Partial derivatives of the multiplication, the chain rule\n",
        "deriv_mul_wrt_x0 = w[0]\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_xw0 * deriv_mul_wrt_x0\n",
        "```"
      ],
      "metadata": {
        "id": "55xdNEuyK4jP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbQl0ODXkHJc"
      },
      "source": [
        "We can now perform the same operation.\n",
        "\n",
        "![](https://drive.google.com/uc?id=13bXiryB5dBMpa9vsAJIDVBJEHOsSO1UM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et4Fo4w5kfyE",
        "outputId": "db3c9ac7-2251-4918-a253-46c3a472c463"
      },
      "source": [
        "\n",
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# -- ----------------------\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the sum, the chain rule\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_sum_wrt_xw1 = 1\n",
        "deriv_sum_wrt_xw2 = 1\n",
        "deriv_sum_wrt_b   = 1\n",
        "\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "deriv_relu_wrt_xw1 = deriv_relu_wrt_z * deriv_sum_wrt_xw1\n",
        "deriv_relu_wrt_xw2 = deriv_relu_wrt_z * deriv_sum_wrt_xw2\n",
        "deriv_relu_wrt_b   = deriv_relu_wrt_z * deriv_sum_wrt_b\n",
        "# -- ----------------------\n",
        "\n",
        "print(f'deriv_relu_wrt_z  : {deriv_relu_wrt_z}')\n",
        "print(f'deriv_relu_wrt_xw0: {deriv_relu_wrt_xw0}')\n",
        "print(f'deriv_relu_wrt_xw1: {deriv_relu_wrt_xw1}')\n",
        "print(f'deriv_relu_wrt_xw2: {deriv_relu_wrt_xw2}')\n",
        "print(f'deriv_relu_wrt_b  : {deriv_relu_wrt_b}')\n",
        "\n",
        "# -- ---------------------------------------\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deriv_relu_wrt_z  : 1.0\n",
            "deriv_relu_wrt_xw0: 1.0\n",
            "deriv_relu_wrt_xw1: 1.0\n",
            "deriv_relu_wrt_xw2: 1.0\n",
            "deriv_relu_wrt_b  : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDfztf39k52T"
      },
      "source": [
        "### Sublayer - weights and inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAD8FKTJkLnY"
      },
      "source": [
        "Continuing backward, the next function is the multiplication of weights and inputs. \n",
        "\n",
        "The derivative for a product is whatever the input is being multiplied by. \n",
        "\n",
        "Recall:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1IAQS4eIH242ZSOLRVK3929RMBosiCx7p)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "![](https://drive.google.com/uc?id=19GdcFSnltR2iSBV2nCL7EMv_YLELhESa)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gv2hEjOlkx5",
        "outputId": "de6af10b-7881-40e9-bc77-e427b3643774"
      },
      "source": [
        "\n",
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "# -- ----------------------\n",
        "# The derivative from the next layer\n",
        "deriv_from_next_layer = 1.0\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Derivative of ReLU and the chain rule\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "# -- ----------------------\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the sum, the chain rule\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_sum_wrt_xw1 = 1\n",
        "deriv_sum_wrt_xw2 = 1\n",
        "deriv_sum_wrt_b   = 1\n",
        "\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "deriv_relu_wrt_xw1 = deriv_relu_wrt_z * deriv_sum_wrt_xw1\n",
        "deriv_relu_wrt_xw2 = deriv_relu_wrt_z * deriv_sum_wrt_xw2\n",
        "deriv_relu_wrt_b   = deriv_relu_wrt_z * deriv_sum_wrt_b\n",
        "# -- ----------------------\n",
        "\n",
        "\n",
        "# -- ----------------------\n",
        "# Partial derivatives of the multiplication, the chain rule\n",
        "deriv_mul_wrt_x0 = w[0]\n",
        "deriv_mul_wrt_x1 = w[1]\n",
        "deriv_mul_wrt_x2 = w[2]\n",
        "\n",
        "deriv_mul_wrt_w0 = x[0]\n",
        "deriv_mul_wrt_w1 = x[1]\n",
        "deriv_mul_wrt_w2 = x[2]\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_xw0 * deriv_mul_wrt_x0\n",
        "deriv_relu_wrt_w0 = deriv_relu_wrt_xw0 * deriv_mul_wrt_w0\n",
        "\n",
        "deriv_relu_wrt_x1 = deriv_relu_wrt_xw1 * deriv_mul_wrt_x1\n",
        "deriv_relu_wrt_w1 = deriv_relu_wrt_xw1 * deriv_mul_wrt_w1\n",
        "\n",
        "deriv_relu_wrt_x2 = deriv_relu_wrt_xw2 * deriv_mul_wrt_x2\n",
        "deriv_relu_wrt_w2 = deriv_relu_wrt_xw2 * deriv_mul_wrt_w2\n",
        "# -- ----------------------\n",
        "\n",
        "\n",
        "print(f'deriv_relu_wrt_z  : {deriv_relu_wrt_z}')\n",
        "print(f'deriv_relu_wrt_xw0: {deriv_relu_wrt_xw0}')\n",
        "print(f'deriv_relu_wrt_xw1: {deriv_relu_wrt_xw1}')\n",
        "print(f'deriv_relu_wrt_xw2: {deriv_relu_wrt_xw2}')\n",
        "print(f'deriv_relu_wrt_b  : {deriv_relu_wrt_b}')\n",
        "print(f'deriv_relu_wrt_x0 : {deriv_relu_wrt_x0}')\n",
        "print(f'deriv_relu_wrt_w0 : {deriv_relu_wrt_w0}')\n",
        "print(f'deriv_relu_wrt_x1 : {deriv_relu_wrt_x1}')\n",
        "print(f'deriv_relu_wrt_w1 : {deriv_relu_wrt_w1}')\n",
        "print(f'deriv_relu_wrt_x2 : {deriv_relu_wrt_x2}')\n",
        "print(f'deriv_relu_wrt_w2 : {deriv_relu_wrt_w2}')\n",
        "\n",
        "# -- ---------------------------------------\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deriv_relu_wrt_z  : 1.0\n",
            "deriv_relu_wrt_xw0: 1.0\n",
            "deriv_relu_wrt_xw1: 1.0\n",
            "deriv_relu_wrt_xw2: 1.0\n",
            "deriv_relu_wrt_b  : 1.0\n",
            "deriv_relu_wrt_x0 : -3.0\n",
            "deriv_relu_wrt_w0 : 1.0\n",
            "deriv_relu_wrt_x1 : -1.0\n",
            "deriv_relu_wrt_w1 : -2.0\n",
            "deriv_relu_wrt_x2 : 2.0\n",
            "deriv_relu_wrt_w2 : 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the graph again: \n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=19GdcFSnltR2iSBV2nCL7EMv_YLELhESa)\n"
      ],
      "metadata": {
        "id": "p571meZcJR0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "LHeeFzoiisJz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url='https://drive.google.com/uc?id=1nSY_g3ksfY70Ui77Eg-L5mBcz4f81ruz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "4CesTGmMihpb",
        "outputId": "5bd03f63-4cd5-4150-f004-af2937c12437"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://drive.google.com/uc?id=1nSY_g3ksfY70Ui77Eg-L5mBcz4f81ruz\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bG9rBLef--V"
      },
      "source": [
        "### Simplify wrt x0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3vY-EsIiF1P"
      },
      "source": [
        "In above code, look how we applied the chain rule to calculate the partial derivative of the ReLU activation function **with respect to the first input**, $x_0$ \n",
        "\n",
        "Let’s take the related lines of the code and simplify what's needed for our final **`deriv_relu_wrt_x0`**:\n",
        "\n",
        "<br>\n",
        "\n",
        "**Original**:\n",
        "\n",
        "```python\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "\n",
        "deriv_mul_wrt_x0 = w[0]\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_xw0 * deriv_mul_wrt_x0\n",
        "```\n",
        "\n",
        "**Replace `deriv_mul_wrt_x0` with `w[0]`**:\n",
        "\n",
        "```python\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "deriv_relu_wrt_xw0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_xw0 * w[0]\n",
        "```\n",
        "\n",
        "\n",
        "**Replace `deriv_relu_wrt_xw0` with `deriv_relu_wrt_z * deriv_sum_wrt_xw0`**:\n",
        "\n",
        "```python\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "deriv_sum_wrt_xw0 = 1\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_z * deriv_sum_wrt_xw0 * w[0]\n",
        "```\n",
        "\n",
        "**Replace `deriv_sum_wrt_xw0` with `1`**:\n",
        "\n",
        "```py\n",
        "deriv_relu_wrt_z = deriv_from_next_layer * (1. if z > 0 else 0.)\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_relu_wrt_z * 1 * w[0]\n",
        "```\n",
        "\n",
        "**Replace `deriv_relu_wrt_z` with `deriv_from_next_layer * (1. if z > 0 else 0.)`**:\n",
        "\n",
        "```python\n",
        "deriv_relu_wrt_x0 = deriv_from_next_layer * (1. if z > 0 else 0.) * w[0]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run again"
      ],
      "metadata": {
        "id": "etC0NMnLO6qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B\"H\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Forward pass\n",
        "x = [1.0, -2.0, 3.0]  # input values\n",
        "w = [-3.0, -1.0, 2.0]  # weights\n",
        "b = 1.0  # bias\n",
        "\n",
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding weighted inputs and a bias\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "# -- ---------------------------------------\n",
        "\n",
        "\n",
        "# -- ---------------------------------------\n",
        "# Backward pass\n",
        "\n",
        "deriv_from_next_layer = 1.0\n",
        "\n",
        "deriv_relu_wrt_x0 = deriv_from_next_layer * (1. if z > 0 else 0.) * w[0]\n",
        "deriv_relu_wrt_x1 = deriv_from_next_layer * (1. if z > 0 else 0.) * w[1]\n",
        "deriv_relu_wrt_x2 = deriv_from_next_layer * (1. if z > 0 else 0.) * w[2]\n",
        "\n",
        "deriv_relu_wrt_w0 = deriv_from_next_layer * (1. if z > 0 else 0.) * x[0]\n",
        "deriv_relu_wrt_w1 = deriv_from_next_layer * (1. if z > 0 else 0.) * x[1]\n",
        "deriv_relu_wrt_w2 = deriv_from_next_layer * (1. if z > 0 else 0.) * x[2]\n",
        "\n",
        "deriv_relu_wrt_b  = deriv_from_next_layer * (1. if z > 0 else 0.) \n",
        "\n",
        "print(f'deriv_relu_wrt_x0 : {deriv_relu_wrt_x0}')\n",
        "print(f'deriv_relu_wrt_w0 : {deriv_relu_wrt_w0}')\n",
        "print(f'deriv_relu_wrt_x1 : {deriv_relu_wrt_x1}')\n",
        "print(f'deriv_relu_wrt_w1 : {deriv_relu_wrt_w1}')\n",
        "print(f'deriv_relu_wrt_x2 : {deriv_relu_wrt_x2}')\n",
        "print(f'deriv_relu_wrt_w2 : {deriv_relu_wrt_w2}')\n",
        "print(f'deriv_relu_wrt_b  : {deriv_relu_wrt_b}')\n",
        "# -- ---------------------------------------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFZyh1qWO7zZ",
        "outputId": "c0314bb4-713e-420e-917b-a1fdcd071d7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deriv_relu_wrt_x0 : -3.0\n",
            "deriv_relu_wrt_w0 : 1.0\n",
            "deriv_relu_wrt_x1 : -1.0\n",
            "deriv_relu_wrt_w1 : -2.0\n",
            "deriv_relu_wrt_x2 : 2.0\n",
            "deriv_relu_wrt_w2 : 3.0\n",
            "deriv_relu_wrt_b  : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the graph again: \n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=19GdcFSnltR2iSBV2nCL7EMv_YLELhESa)"
      ],
      "metadata": {
        "id": "PQ6pmo1dP3G5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUUl3vyJXF4i"
      },
      "source": [
        "### The Gradients\n",
        "\n",
        "The partial derivatives above, combined into a vector, make up our gradients. \n",
        "\n",
        "Our gradients could be represented as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSSHRJBLnk5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a98208b-af27-4b99-8f49-e8577460444e"
      },
      "source": [
        "# gradients on inputs:\n",
        "x_gradient = [deriv_relu_wrt_x0, deriv_relu_wrt_x1, deriv_relu_wrt_x2]  \n",
        "\n",
        "# gradients on weights:\n",
        "w_gradient = [deriv_relu_wrt_w0, deriv_relu_wrt_w1, deriv_relu_wrt_w2]  \n",
        "\n",
        "# gradient on bias...just 1 bias here:\n",
        "b_gradient = deriv_relu_wrt_b  \n",
        "\n",
        "print(f'x_gradient: {x_gradient}')\n",
        "print(f'w_gradient: {w_gradient}')\n",
        "print(f'b_gradient: {b_gradient}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_gradient: [-3.0, -1.0, 2.0]\n",
            "w_gradient: [1.0, -2.0, 3.0]\n",
            "b_gradient: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKPn1Ubsn_hV"
      },
      "source": [
        "> NOTE: \n",
        "> - For this single neuron example, we also won’t need our `x_gradient`. \n",
        "> - With many layers, we will continue backpropagating to preceding layers with the partial derivative with respect to our inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOmaNFaUocoj"
      },
      "source": [
        "### Apply gradients to the weights\n",
        "\n",
        "We can now apply these gradients to the weights to hopefully minimize the output. \n",
        "\n",
        "This is typically the purpose of the **optimizer** (discussed in following sections), but we can show a simplified version of this task by directly applying a negative fraction of the gradient to our weights. \n",
        "\n",
        "We apply a **negative** fraction to this gradient since we want to decrease the final output value, and the gradient shows the direction of the steepest ascent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVE0KNobo9Zd",
        "outputId": "88042fc9-ee65-453b-de26-14929bb0717c"
      },
      "source": [
        "# Our current weights and bias are:\n",
        "print(f'w: {w}')\n",
        "print(f'b: {b}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w: [-3.0, -1.0, 2.0]\n",
            "b: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NVkLG4UpPeF",
        "outputId": "55a79a0e-45c9-4cac-f990-7a5e6a724414"
      },
      "source": [
        "# Our gradients:\n",
        "print(f'w_gradient: {w_gradient}')\n",
        "print(f'b_gradient: {b_gradient}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_gradient: [1.0, -2.0, 3.0]\n",
            "b_gradient: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pN9vIKOoyB_"
      },
      "source": [
        "We can then apply a fraction of the gradients to these values:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w[0] += -0.001 * w_gradient[0]\n",
        "w[1] += -0.001 * w_gradient[1]\n",
        "w[2] += -0.001 * w_gradient[2]\n",
        "b += -0.001 * b_gradient\n",
        "\n",
        "print(w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfvEd7uHRMsh",
        "outputId": "ad47d098-c228-486f-c2ec-07fa9fc9cf72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.001, -0.998, 1.997] 0.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3rH-EDapXju"
      },
      "source": [
        "### Run another forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcCWxCutpl-z",
        "outputId": "a739fec4-cc1f-4b9b-d7cb-e45e7d243e9f"
      },
      "source": [
        "# Multiplying inputs by weights\n",
        "xw0 = x[0] * w[0]\n",
        "xw1 = x[1] * w[1]\n",
        "xw2 = x[2] * w[2]\n",
        "\n",
        "# Adding\n",
        "z = xw0 + xw1 + xw2 + b\n",
        "\n",
        "# ReLU activation function\n",
        "y = max(z, 0)\n",
        "print(y)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ve successfully decreased this neuron’s output from 6.000 to 5.985.\n",
        "\n",
        "Note that it does not make sense to decrease the neuron’s output in a real neural network; we were doing this **purely as a simpler exercise than the full network**. "
      ],
      "metadata": {
        "id": "PouX8MskRZeT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "875vVOoY4fWW"
      },
      "source": [
        "## More Complex Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prRXQ0p94hzC"
      },
      "source": [
        "### Intro\n",
        "\n",
        "\n",
        "<u>**Scenario A: 1 neuron in current layer, 1 in next layer**</u>\n",
        "\n",
        "So far, we have performed an example backward pass with a **single neuron**, which received a **singular derivative**, `deriv_from_next_layer` (from \"next\" layer) to apply the chain rule. \n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<u>**Scenario B: 1 neuron in current layer, mulitple in next layer**</u>\n",
        "\n",
        "Let’s consider **multiple neurons** in the next layer. A **single neuron** of the current layer connects to all of them — they all receive the output of this neuron. \n",
        "\n",
        "What will happen during backpropagation? \n",
        "- Each neuron from the next layer will return a partial derivative of its function with respect to this input. \n",
        "- The neuron in the current layer will receive a vector consisting of these derivatives. \n",
        "- We need this to be a **singular value** for a singular neuron. \n",
        "- To continue backpropagation, we need to **sum** this vector.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<u>**Scenario C: Multiple neurons in current layer, mulitple in next layer**</u>\n",
        "\n",
        "During backpropagation: \n",
        "- Each neuron from the current layer will receive a vector of partial derivatives the same way that we described for Scenario B. \n",
        "- With a layer of neurons, it’ll take the form of a list of these vectors, or a 2D array. \n",
        "- Each neuron in the next layer is going to output a gradient of the partial derivatives with respect to all of its inputs. \n",
        "- From all the neurons in the next layer this will form a list of these vectors. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al3mmHX1pz2H"
      },
      "source": [
        "### Our staged scenario\n",
        "\n",
        "We're zooming into a layer of 3 neurons with 4 inputs. Let's image we have done the forward pass already and are in middle of the backpropagation and have the gradients for the \"next layer/sublayer\" already. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpYyDAb-JdL8"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvDuHyHDKj8A"
      },
      "source": [
        "#### Current layer and gradient from next"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgdB5qYUuY9q"
      },
      "source": [
        "# -- ------------------------------\n",
        "# Passed-in gradient from the next layer/sublayer\n",
        "dvalues = np.array([[1., 1., 1.]])\n",
        "\n",
        "# We have 3 sets of weights - one set for each neuron\n",
        "# we have 4 inputs, thus 4 weights\n",
        "# recall that we keep weights transposed\n",
        "weights = np.array([\n",
        "    [0.2, 0.8, -0.5, 1],       # weights for neuron 1\n",
        "    [0.5, -0.91, 0.26, -0.5],  # weights for neuron 2\n",
        "    [-0.26, -0.27, 0.17, 0.87] # weights for neuron 3\n",
        "]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZnig9bnNnYo",
        "outputId": "593698bb-2624-4530-80c9-4903ef05926f"
      },
      "source": [
        "dvalues"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c97l2K7qNovl",
        "outputId": "9b33f420-c1a4-47df-8c7d-91b253627be0"
      },
      "source": [
        "dvalues.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgXLSfVLODwD"
      },
      "source": [
        "<u>**Remember:**</u> `weights` has been transposed:\n",
        "- Before each \"record\" represented a neuron and its weights\n",
        "- Now each \"record\" represents an input attribute, of which there are 4, and its 3 weights (one for each neuron) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cS5wtl3Nqll",
        "outputId": "d58cdf91-edcd-49a1-d032-cca57ecc127c"
      },
      "source": [
        "weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.2 ,  0.5 , -0.26],\n",
              "       [ 0.8 , -0.91, -0.27],\n",
              "       [-0.5 ,  0.26,  0.17],\n",
              "       [ 1.  , -0.5 ,  0.87]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAfLDLL5NvmF",
        "outputId": "77dbbae1-4ed6-496d-b671-6cea6e325f77"
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24n6KFV3uRGW"
      },
      "source": [
        "#### Get gradient\n",
        "\n",
        "Remember from above, in section _**Sublayer - weights and inputs**_, that to calculate the partial derivative **with respect to the input** equals the related weight. \n",
        "\n",
        "> Note: `dinputs` is a gradient of the neuron function with respect to inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwAJjjMDKu9G",
        "outputId": "1c787f1f-1e13-487a-a555-07c65097da5b"
      },
      "source": [
        "# Sum weights related to the given input multiplied by\n",
        "# the gradient related to the given neuron\n",
        "dx0 = sum([\n",
        "    weights[0][0]*dvalues[0][0], \n",
        "    weights[0][1]*dvalues[0][1],\n",
        "    weights[0][2]*dvalues[0][2]\n",
        "])\n",
        "\n",
        "dx1 = sum([\n",
        "    weights[1][0]*dvalues[0][0], \n",
        "    weights[1][1]*dvalues[0][1],\n",
        "    weights[1][2]*dvalues[0][2]\n",
        "])\n",
        "\n",
        "dx2 = sum([\n",
        "    weights[2][0]*dvalues[0][0], \n",
        "    weights[2][1]*dvalues[0][1],\n",
        "    weights[2][2]*dvalues[0][2]\n",
        "])\n",
        "\n",
        "dx3 = sum([\n",
        "    weights[3][0]*dvalues[0][0], \n",
        "    weights[3][1]*dvalues[0][1],\n",
        "    weights[3][2]*dvalues[0][2]\n",
        "])\n",
        "\n",
        "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
        "\n",
        "print(dinputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.44 -0.38 -0.07  1.37]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ziNIOuXPghT"
      },
      "source": [
        "#### Using dot product\n",
        "\n",
        "We can achieve the same result by using the `np.dot`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ56ACyVPxd0",
        "outputId": "23002a77-fc55-4a36-a1a3-f3c6d6c795e5"
      },
      "source": [
        "# sum weights of given input\n",
        "# and multiply by the passed-in gradient for this neuron\n",
        "dinputs = np.dot(dvalues[0], weights.T)\n",
        "\n",
        "print(dinputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.44 -0.38 -0.07  1.37]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA4sk1OlQG84"
      },
      "source": [
        "#### Using batch of inputs\n",
        "\n",
        "With more samples, the \"next\" layer will return a list of gradients. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjXJOL8Sfbh"
      },
      "source": [
        "##### wrt inputs\n",
        "\n",
        "Our code just needs a minor tweak: `dvalues` instead of `dvalues[0]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4-ytPe3QKR8",
        "outputId": "ef8c6412-b7e0-4f42-b42d-2bbf1d6fad4b"
      },
      "source": [
        "# Passed-in gradient from the next layer\n",
        "# for the purpose of this example we're going to use\n",
        "# an array of an incremental gradient values\n",
        "dvalues = np.array([[1., 1., 1.],\n",
        "                    [2., 2., 2.],\n",
        "                    [3., 3., 3.]])\n",
        "\n",
        "# SAME AS BEFORE\n",
        "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
        "                    [0.5, -0.91, 0.26, -0.5],\n",
        "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
        "\n",
        "# sum weights of given input\n",
        "# and multiply by the passed-in gradient for this neuron\n",
        "dinputs = np.dot(dvalues, weights.T)\n",
        "\n",
        "print(dinputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.44 -0.38 -0.07  1.37]\n",
            " [ 0.88 -0.76 -0.14  2.74]\n",
            " [ 1.32 -1.14 -0.21  4.11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6sJIbJDQjXJ"
      },
      "source": [
        "##### wrt weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DueBaGcWSsIO"
      },
      "source": [
        "# We have 3 sets of inputs - samples\n",
        "inputs = np.array([[1, 2, 3, 2.5],\n",
        "                   [2., 5., -1., 2],\n",
        "                   [-1.5, 2.7, 3.3, -0.8]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w95v6dmuT6pu",
        "outputId": "98d43937-abcc-4634-ff7b-975c0be105ee"
      },
      "source": [
        "# Let's remember what dvalues looks like:\n",
        "dvalues"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1.],\n",
              "       [2., 2., 2.],\n",
              "       [3., 3., 3.]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7XXvCkTxeN",
        "outputId": "d541d20c-eea2-414a-ffaa-a4847a87bd54"
      },
      "source": [
        "# inputs transposed (imagine now that \"row\" one has all the 3 eyecolors of the 3 people being inputted)\n",
        "inputs.T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1. ,  2. , -1.5],\n",
              "       [ 2. ,  5. ,  2.7],\n",
              "       [ 3. , -1. ,  3.3],\n",
              "       [ 2.5,  2. , -0.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgYikW57TwcU",
        "outputId": "c4bb74e4-de5b-4c16-f4a1-ff001d46710c"
      },
      "source": [
        "# sum inputs for given weight\n",
        "# and multiply by the passed-in gradient for this neuron\n",
        "dweights = np.dot(inputs.T, dvalues)\n",
        "\n",
        "print(dweights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.5  0.5  0.5]\n",
            " [20.1 20.1 20.1]\n",
            " [10.9 10.9 10.9]\n",
            " [ 4.1  4.1  4.1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v_HqRSFSzse"
      },
      "source": [
        "##### wrt to biases\n",
        "\n",
        "For the biases and derivatives with respect to them, the derivatives come from the sum operation and always equal 1, multiplied by the incoming gradients to apply the chain rule. \n",
        "\n",
        "Since gradients are a list of gradients (a vector of gradients for each neuron for all samples), we just have to sum them with the neurons, column-wise, along axis 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6ZQ8sRcUu0f",
        "outputId": "212c5332-53f4-42d6-b344-3c8b81f14b4b"
      },
      "source": [
        "# One bias for each neuron\n",
        "# biases are the row vector with a shape (1, neurons)\n",
        "biases = np.array([[2, 3, 0.5]])\n",
        "\n",
        "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "print(dbiases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6. 6. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDoRTSE2UbaW"
      },
      "source": [
        "##### Sublayer ReLU\n",
        "\n",
        "Let's now move up to the next sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8EH2-SwVIKw"
      },
      "source": [
        "# Example layer output (before ReLU)\n",
        "z = np.array([[1, 2, -3, -4],\n",
        "              [2, -7, -1, 3],\n",
        "              [-1, 2, 5, -1]])\n",
        "\n",
        "# Gradient from \"next\" layer\n",
        "dvalues = np.array([[1, 2, 3, 4],\n",
        "                    [5, 6, 7, 8],\n",
        "                    [9, 10, 11, 12]])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTAJMDO0avEn",
        "outputId": "7dd3d6cf-ba39-4825-8183-23fab64ef30e"
      },
      "source": [
        "# ReLU activation's derivative\n",
        "drelu = np.zeros_like(z)\n",
        "# Set cells to 1 where corresponding z cell > 0\n",
        "drelu[z > 0] = 1\n",
        "\n",
        "print(drelu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 0 0]\n",
            " [1 0 0 1]\n",
            " [0 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4Xwbn8az9A",
        "outputId": "833cb1bf-24e4-4f4e-9845-2659f83136ef"
      },
      "source": [
        "# The chain rule\n",
        "drelu *= dvalues\n",
        "\n",
        "print(drelu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  2  0  0]\n",
            " [ 5  0  0  8]\n",
            " [ 0 10 11  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esaKECPJbO_Y"
      },
      "source": [
        "###### Simplified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAM1F5HmbhWL",
        "outputId": "c1e72996-8820-4a00-9135-cd2d0db07cc1"
      },
      "source": [
        "drelu = dvalues.copy()\n",
        "drelu[z <= 0] = 0\n",
        "\n",
        "print(drelu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  2  0  0]\n",
            " [ 5  0  0  8]\n",
            " [ 0 10 11  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-BbjZ7Ialdu"
      },
      "source": [
        "#### Full forward and backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALfHZvs2cEdq"
      },
      "source": [
        "# Passed-in gradient from the next layer\n",
        "# for the purpose of this example we're going to use\n",
        "# an array of an incremental gradient values\n",
        "dvalues = np.array([[1., 1., 1.],\n",
        "                    [2., 2., 2.],\n",
        "                    [3., 3., 3.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlSRP4mWcHMK"
      },
      "source": [
        "# We have 3 sets of inputs - samples\n",
        "inputs = np.array([[1, 2, 3, 2.5],\n",
        "                   [2., 5., -1., 2],\n",
        "                   [-1.5, 2.7, 3.3, -0.8]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2LHevTRcJU9"
      },
      "source": [
        "# We have 3 sets of weights - one set for each neuron\n",
        "# we have 4 inputs, thus 4 weights\n",
        "# recall that we keep weights transposed\n",
        "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
        "                    [0.5, -0.91, 0.26, -0.5],\n",
        "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
        "\n",
        "\n",
        "# One bias for each neuron\n",
        "biases = np.array([[2, 3, 0.5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPqZ_rE2cSXX"
      },
      "source": [
        "# Forward pass\n",
        "layer_outputs = np.dot(inputs, weights) + biases  # Dense layer\n",
        "relu_outputs = np.maximum(0, layer_outputs)  # ReLU activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JHxrzSqcUcv"
      },
      "source": [
        "# Backpropagation starts here\n",
        "\n",
        "# ReLU activation - simulates derivative with respect to input values\n",
        "# from next layer passed to current layer during backpropagation\n",
        "drelu = relu_outputs.copy()\n",
        "drelu[layer_outputs <= 0] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJw1Ickpchq-"
      },
      "source": [
        "# dinputs - multiply by weights\n",
        "dinputs = np.dot(drelu, weights.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA-Ri-QkclCw"
      },
      "source": [
        "# dweights - multiply by inputs\n",
        "dweights = np.dot(inputs.T, drelu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-XOgvoPcmdf"
      },
      "source": [
        "# dbiases - sum values, do this over samples (first axis)\n",
        "dbiases = np.sum(drelu, axis=0, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CZMw6Cob3js",
        "outputId": "2dea6ce8-5c47-4b98-e1cb-4be481b89623"
      },
      "source": [
        "# Update parameters\n",
        "weights += -0.001 * dweights\n",
        "biases += -0.001 * dbiases\n",
        "\n",
        "print(weights)\n",
        "print(biases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.179515   0.5003665 -0.262746 ]\n",
            " [ 0.742093  -0.9152577 -0.2758402]\n",
            " [-0.510153   0.2529017  0.1629592]\n",
            " [ 0.971328  -0.5021842  0.8636583]]\n",
            "[[1.98489  2.997739 0.497389]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l1l74qialiI"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "By this point, we’ve covered everything we need to perform backpropagation, except for the derivative of the Softmax activation function and the derivative of the cross-entropy loss function.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}