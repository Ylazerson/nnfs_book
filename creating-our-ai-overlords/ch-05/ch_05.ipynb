{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M0umBngSVA2"
      },
      "source": [
        "# B\"H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8pKSYhvSWjj"
      },
      "source": [
        "## Calculating Network Error with Loss\n",
        "\n",
        "The **loss function** is also referred to as the **cost function**\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "The output of our neural network is confidence, and more confidence in the correct answer is better. Because of this, we strive to _increase correct confidence_ and _decrease misplaced confidence_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQQR1-xWTHW3"
      },
      "source": [
        "## Categorical Cross-Entropy Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzN74Ef3TLcc"
      },
      "source": [
        "Our model has a softmax activation function for the output layer, which means\n",
        "it’s outputting a probability distribution. \n",
        "\n",
        "**Categorical cross-entropy** is explicitly used to compare a “ground-truth” probability (y or “targets”) and some predicted distribution (y-hat or\n",
        "“predictions”)\n",
        "\n",
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=1sk7Zb-OCV3W7vx2a5qQv7HbReTbbMqHZ)\n",
        "\n",
        "![](https://drive.google.com/uc?id=1qhlJBbRf_oARGJUWPWEgvJpeBnnCfW9b)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "We’ll simplify it further to `-log(correct_class_confidence)`:\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://drive.google.com/uc?id=15cYt14T0YC8MJftsWwVTSseB2yHbNZwv)\n",
        "\n",
        "![](https://drive.google.com/uc?id=1Y0wCm90FkvXUrpROnzjix85uXSdVEKJA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-GgK7G6XXq4"
      },
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "Let's use the following for an example:\n",
        "\n",
        "- Softmax output: `[0.7, 0.1, 0.2]` \n",
        "- Target probability distribution: `[1, 0, 0]`\n",
        "\n",
        "Note, cross-entropy can also work on target probability distributions like `[0.2, 0.5, 0.3]`.\n",
        "\n",
        "When comparing the model’s results to a one-hot vector (as in our case), the other parts of the equation zero out, making the cross-entropy calculation relatively simple. \n",
        "\n",
        "This is also a special case of the cross-entropy calculation, called **categorical cross-entropy**. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l60afdAa3yp"
      },
      "source": [
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYRj-jIsbKA6"
      },
      "source": [
        "softmax_output = [0.7, 0.1, 0.2]\n",
        "target_output = [1, 0, 0]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA8fRoB9ZsiQ",
        "outputId": "25e6405a-64c7-4cd1-c99a-a7c3e061bc37"
      },
      "source": [
        "loss = -(\n",
        "    math.log(softmax_output[0])*target_output[0] +\n",
        "    math.log(softmax_output[1])*target_output[1] +\n",
        "    math.log(softmax_output[2])*target_output[2]\n",
        ")\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35667494393873245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh6IyTnzX-He"
      },
      "source": [
        "That’s the full categorical cross-entropy calculation, but let's simplify it. \n",
        "\n",
        "- Anything multiplied by 0 is 0. Thus, we don’t need to calculate these indices. \n",
        "- Any number multiplied by 1 remains the same. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIosZcNPW3Yg",
        "outputId": "963bc052-752e-4e9e-9ae9-1e2fdb3ab6fe"
      },
      "source": [
        "correct_idx = 0\n",
        "\n",
        "loss = -math.log(softmax_output[correct_idx])\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35667494393873245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi_OvslJTpk2"
      },
      "source": [
        "--- \n",
        "\n",
        "<br><br>\n",
        "\n",
        "The **Categorical Cross-Entropy Loss** is cool - returns larger loss for lower confidence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiFi7RcVcqcT",
        "outputId": "0e2d129a-86d7-46d1-9557-0932ae24595e"
      },
      "source": [
        "print(\"1.     :\", -math.log(1.))\n",
        "print(\"0.95   :\", -math.log(0.95))\n",
        "print(\"0.9    :\", -math.log(0.9))\n",
        "print(\"0.8    :\", -math.log(0.8))\n",
        "print(\"0.7    :\", -math.log(0.7))\n",
        "print(\"0.6    :\", -math.log(0.6))\n",
        "print(\"0.5    :\", -math.log(0.5))\n",
        "print(\"0.4    :\", -math.log(0.4))\n",
        "print(\"0.3    :\", -math.log(0.3))\n",
        "print(\"0.2    :\", -math.log(0.2))\n",
        "print(\"0.1    :\", -math.log(0.1))\n",
        "print(\"0.05   :\", -math.log(0.05))\n",
        "print(\"0.01   :\", -math.log(0.01))\n",
        "print(\"0.001  :\", -math.log(0.001))\n",
        "print(\"0.00001:\", -math.log(0.00001))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.     : -0.0\n",
            "0.95   : 0.05129329438755058\n",
            "0.9    : 0.10536051565782628\n",
            "0.8    : 0.2231435513142097\n",
            "0.7    : 0.35667494393873245\n",
            "0.6    : 0.5108256237659907\n",
            "0.5    : 0.6931471805599453\n",
            "0.4    : 0.916290731874155\n",
            "0.3    : 1.2039728043259361\n",
            "0.2    : 1.6094379124341003\n",
            "0.1    : 2.3025850929940455\n",
            "0.05   : 2.995732273553991\n",
            "0.01   : 4.605170185988091\n",
            "0.001  : 6.907755278982137\n",
            "0.00001: 11.512925464970229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABO-AymiSmk7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}