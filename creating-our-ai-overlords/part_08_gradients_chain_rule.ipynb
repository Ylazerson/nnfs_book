{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradients_partial_derivatives_chain_rule.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtpZ8319mkhH"
      },
      "source": [
        "# B\"H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT2dCLSfmnD3"
      },
      "source": [
        "## The Partial Derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNfVtTMKm3ZU"
      },
      "source": [
        "### Intro\n",
        "\n",
        "The **partial derivative** measures how much impact a **single** input has on a function’s output. \n",
        "\n",
        "The method for calculating a partial derivative is the **same** as for derivatives explained in the previous chapter; we simply have to repeat this process for each of the independent inputs.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "We have to calculate the derivative with respect to each input **separately** to learn about each of them. That’s why we call these partial derivatives **with respect to given input**. \n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "The partial derivative is a **single equation**, and the full multivariate function’s derivative consists of a set of equations called the **gradient**. \n",
        "\n",
        "The gradient is a **vector**, of the size of inputs, containing partial derivative solutions with respect to each of the inputs.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "To denote the partial derivative, we’ll be using Euler’s notation. It’s very similar to Leibniz’s notation, as we only need to replace the differential operator $\\large d$ with $\\large ∂$. \n",
        "\n",
        "> While the $\\large d$ operator might be used to denote the differentiation of a multivariate function, its meaning is a bit different — it can mean the rate of the function’s change in relation to the given input, but **when other inputs might change as well**, and it is used mostly in physics. \n",
        "> \n",
        "> We are interested in the partial derivatives, a situation where we try to find the impact of the given input to the output **while treating all of the other inputs as constants**. We are interested in the impact of singular inputs since our goal, in the model, is to update parameters. The $\\large ∂$ operator means explicitly that — the partial derivative.\n",
        "\n",
        "![](https://drive.google.com/uc?id=11Y3HORRpUJeOfxRz9lIe-iAfssUgQlIQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P7XaLPYo4QJ"
      },
      "source": [
        "### Partial Derivative of a Sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPa28w56pAnm"
      },
      "source": [
        "#### Example 1\n",
        "\n",
        "![](https://drive.google.com/uc?id=1BEQjT_gFJGBIYr2hEkwoa370aBmgy6iY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBEu9PTXsKHm"
      },
      "source": [
        "#### Example 2\n",
        "\n",
        "![](https://drive.google.com/uc?id=1rpGJsbz6CWgWnlhvf5kV30obmSXERRQk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOkoFH-ZtEv5"
      },
      "source": [
        "### Partial Derivative of Multiplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1egimdYptTbv"
      },
      "source": [
        "#### Example 1\n",
        "\n",
        "![](https://drive.google.com/uc?id=1-4bOjxzAB_nWlqMwEYXfKuxGbQb_ap-5)\n",
        "\n",
        "We have already mentioned that we need to treat the other independent variables as constants, and we also have learned that we can move constants to the outside of the derivative. \n",
        "\n",
        "That’s exactly how we solve the calculation of the partial derivative of multiplication — we treat other variables as constants and move them outside of the derivative. \n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The intuition, when calculating the partial derivative with respect to $\\large x$: \n",
        "- every change of $\\large x$ by 1 changes the function’s output by $\\large y$. \n",
        "- For example: \n",
        "    - If $y=3$ and $x=1$, the result is $1·3=3$. \n",
        "    - When we change $x$ by $1$ so $y=3$ and $x=2$, the result is $2·3=6$. \n",
        "    - We changed $x$ by $1$ and the result changed by $3$, by the $y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdmQjvl8t55q"
      },
      "source": [
        "#### Example 2\n",
        "\n",
        "This is a longer example but not more complicated.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=16HhEK2G39JoJU7yP4rXnmdS7-JEQDbQb)\n",
        "\n",
        "---\n",
        "\n",
        "![](https://drive.google.com/uc?id=1oJMWNOz4wbRZbvVQ-wTfluabTEK9MPUu)\n",
        "\n",
        "---\n",
        "\n",
        "![](https://drive.google.com/uc?id=10NmqfFM_bRCE345FGkcDsD1hDaeLq37D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNTd6hUpvtOV"
      },
      "source": [
        "### Partial Derivative of Max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqRQfqJowYsn"
      },
      "source": [
        "We know that the derivative of x **with respect to x** equals 1, so the derivative of this function with respect to x equals 1 if x is greater than y, since the function will return x. \n",
        "\n",
        "In the other case, where y is greater than x and will get returned instead, the derivative of max() **with respect to x** equals 0 — we treat y as a constant, and the derivative of y **with respect to x** equals 0. \n",
        "\n",
        "![](https://drive.google.com/uc?id=1p8sfbRH0Hwj1cJHQsAobC-qW4T6AOPdw)\n",
        "\n",
        "We can denote that as 1(x > y), which means 1 if the condition is met, and 0 otherwise. \n",
        "\n",
        "> **Note:** we could also calculate the partial derivative of max() **with respect to y**, but we won’t need it anywhere in this book.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhwEddZlxlHm"
      },
      "source": [
        "### Partial Derivative of Max with Constant\n",
        "\n",
        "![](https://drive.google.com/uc?id=1Tn5WUOU-3h8oD7WdfVdDGtnJs8ec3V8m)\n",
        "\n",
        "As above, the derivative is $1$ when $x$ is greater than $0$, otherwise, it’s $0$.\n",
        "\n",
        "\n",
        "Notice that since this function takes a **single parameter**, we used the $\\large d$ operator instead of the $\\large ∂$ to calculate the non-partial derivative. \n",
        "\n",
        "\n",
        "Handling this is going to be useful when we calculate the derivative of the ReLU activation function since that activation function is defined as $max(x, 0)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSmwPiVjyiQe"
      },
      "source": [
        "## The Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_3VkJh3yo8A"
      },
      "source": [
        "As we mentioned at the beginning of this chapter, the **gradient** is a **vector** composed of all of the partial derivatives of a function, calculated with respect to each input variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxA2Qe3kyxe6"
      },
      "source": [
        "### Example\n",
        "\n",
        "![](https://drive.google.com/uc?id=1i4CiZ_4nEdDS-_ljBuGaoig4Eg0QnOiM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zB4Kq1BzaJ1"
      },
      "source": [
        "That’s all we have to know about the gradient - it’s a vector of all of the possible partial derivatives of the function, and we denote it using the $\\large ∇$ — nabla symbol that looks like an inverted delta symbol.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "We’ll be using **derivatives** of single-parameter functions and **gradients** of multivariate functions to perform **gradient descent** using the **chain rule**, or, in other words, to perform the **backward pass**, which is a part of the model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-6Yb86Oz3v_"
      },
      "source": [
        "## The Chain Rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpK2dqbNz_t5"
      },
      "source": [
        "### Intro\n",
        "\n",
        "Let’s take 2 functions:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1TPkvGXoUGU8INubvqDs0uy88MKJj4gpJ)\n",
        "\n",
        "<br>\n",
        "\n",
        "We could write the same calculation as:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1f-t6NRPXHmQ_72Pu95iKOLY4vyZn611s)\n",
        "\n",
        "<br>\n",
        "\n",
        "The output of the function $g$ is influenced by $x$ in some way, so there must exist a derivative which can inform us of this influence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctKXO6G6374Y"
      },
      "source": [
        "### The Why\n",
        "\n",
        "We can look at the **loss function** not only as a function that takes the **model’s output and targets as parameters** to produce the error, but also as a function that takes **targets, samples, and all of the weights and biases as inputs** if we chain all of the functions performed during the forward pass. \n",
        "\n",
        "To improve loss, we need to learn how each weight and bias impacts it. How to do that for a chain of functions? By using the chain rule.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "The chain rule turns out to be the most important rule in finding the **impact of singular input to the output of a chain of functions**, which is the calculation of loss in our case. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uNjtnnb4rvO"
      },
      "source": [
        "### The Rule\n",
        "\n",
        "This Chain Rule: **the derivative of a function chain** $\\large \\large =$ **the _product_ of derivatives of _all_ of the functions in this chain**.\n",
        "\n",
        "For example:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1cEMUvaXaFihOYWWnv36pm6FRkz3M6Rhh)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0Qx0fTq5uOU"
      },
      "source": [
        "### With Multiple Inputs\n",
        "\n",
        "Here's an example of 3 functions and multiple inputs. \n",
        "\n",
        "We'll show the partial derivative of this function **with respect to x** \n",
        "\n",
        "> **IMPORTANT NOTE**: we can’t use the prime ($\\prime$) notation in this case since we have to mention which variable we are deriving with respect to.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1PmtF49J2vCnDym7f5loQ6nEWjZW8fFy4)\n",
        "\n",
        "<br>\n",
        "\n",
        "Here is the function def in pseudo-code:\n",
        "\n",
        "```\n",
        "f(\n",
        "    g(\n",
        "        y,\n",
        "        h(x,z)          \n",
        "    )    \n",
        ")\n",
        "```\n",
        "\n",
        "> **IMPORTANT NOTE**: \n",
        ">\n",
        "> Looking at the above pseudo-code we can now see why the middle derivative is with respect to $h(x, z)$ and not $y$.\n",
        ">\n",
        "> This is because $h(x, z)$ is in the chain to the parameter $x$. \n",
        ">\n",
        "> This idea will become more clear as we work thru some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HBRcvhV73aF"
      },
      "source": [
        "### Example\n",
        "\n",
        "Let’s solve the derivative of $\\large h(x) = 3(2x^2)^5$ \n",
        "\n",
        "The first thing that we can notice here is that we have a complex function that can be split into two simpler functions: \n",
        "- $\\large g(x) = 2x^2$ \n",
        "- $\\large f(y) = 3(y)^5$\n",
        "- i.e. $\\large y$ is $\\large g(x)$\n",
        "\n",
        "In short: $\\large h(x) = f(g(x)) = 3(2x^2)^5$ \n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "Let's solve it.\n",
        "\n",
        "**Step 1 - Get outer function derivative**\n",
        "- Don't touch what's inside the parentheses, i.e. the inner function.\n",
        "\n",
        "![](https://drive.google.com/uc?id=11U_LvFnYCiwKIB-FwuKfvIXdxYzeQreo)\n",
        "\n",
        "**Step 2 - Multiply above derivative with the derivative of the interior function**\n",
        "\n",
        "![](https://drive.google.com/uc?id=19ycpI14ZCAFHv-NZ7hmJx2CcVlt3Hlr7)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "In theory, we could just stop here. We can enter some input into $15(2x^2)^4 · 4x$ and get the answer. \n",
        "\n",
        "That said, let's simplify:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1aaNhSmUUUrz9jtzHRNgx9EPX5ONaiIVk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O5FLQRSYpyi"
      },
      "source": [
        "#### Alternative\n",
        "\n",
        "Alternatively, we **do NOT** have to treat it as two functions, but rather just keep it as one single function.\n",
        "\n",
        "Let's solve the derivative of $\\large h(x) = 3(2x^2)^5$ without separating it into two functions. \n",
        "\n",
        "---\n",
        "\n",
        "**Step 1 - Simplify Expresssion**\n",
        "\n",
        "> $\\large 3(2x^2)^5$\n",
        ">\n",
        "> $\\large = 3 * 2^5  x^{5*2}$\n",
        ">\n",
        "> $\\large = 3 * 32  x^{10}$\n",
        ">\n",
        "> $\\large = 96  x^{10}$\n",
        "\n",
        "**Step 2 - Solve Derivative**\n",
        "\n",
        "> $\\large f(x) = 96  x^{10}$\n",
        ">\n",
        "> $\\large f^\\prime(x) = \\frac{d}{dx}  96  x^{10}$\n",
        ">\n",
        "> $\\large f^\\prime(x) = 10*96  x^{9}$\n",
        ">\n",
        "> $\\large f^\\prime(x) = 960  x^{9}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riEGNBp_YsWE"
      },
      "source": [
        "#### IMPORTANT NOTE\n",
        "\n",
        "In other words, the **Chain Rule** is a tool that can be very helpful at times - that does not mean it is strictly required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXTiLiIsfvkq"
      },
      "source": [
        "### When to use/not use\n",
        "\n",
        "See [here](https://socratic.org/questions/when-do-you-use-the-chain-rule-instead-of-the-product-rule) for details.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "**Explanation 1**\n",
        "\n",
        "![](https://drive.google.com/uc?id=14lvO4TxcFDibRtR1ASsVZEyWrdxG0IsN)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Explanation 2**\n",
        "\n",
        "![](https://drive.google.com/uc?id=1BwuD4azFg2yA3kj8TDBIe1gaGfokvypU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqvVSZrShvez"
      },
      "source": [
        "#### Side note\n",
        "\n",
        "- Reminder of [Product Rule](https://socratic.org/calculus/basic-differentiation-rules/product-rule)\n",
        "- Reminder of [Quotient Rule](https://socratic.org/calculus/basic-differentiation-rules/quotient-rule)"
      ]
    }
  ]
}